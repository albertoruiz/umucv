{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geometría visual del plano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los conceptos esenciales que estudiaremos en este tema son:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Las transformaciones que sufren las escenas planas cuando se observan en una imagen se representan con matrices (3x3) llamadas \"homografías\", que actúan sobre las coordenadas homogéneas de los puntos.\n",
    "\n",
    "\n",
    "- Una homografía H se aplica a todos los puntos de una imagen de forma eficiente con la función `cv.warpPerspective`.\n",
    "\n",
    "\n",
    "- Las homografías se deducen del efecto que produce la transformación en (al menos) 4 puntos. Para ello utilizamos `cv.findHomograpy`.\n",
    "\n",
    "Esto permite rectificar planos (deshacer la perspectiva y recuperar las medidas reales) si detectamos un objeto conocido en la escena. También podemos crear diferentes efectos basados en la transformación de cuadriláteros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"../images/demos/rectifbig.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grupos de transformaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las transformaciones más importantes de la geometría visual se organizan en grupos progresivamente más generales:\n",
    "\n",
    "<img src=\"../images/demos/transf2d.svg\" width=\"600px\"/>\n",
    "\n",
    "Todas ellas pueden expresarse de forma lineal (multiplicando por una determinada matriz) usando coordenadas homogéneas. Las ilustramos en el plano (matrices $3\\times3$), pero el concepto se aplica igualmente al espacio 3D con matrices $4\\times4$).\n",
    "\n",
    "\n",
    "Las transformaciones **euclídeas** (o rígidas) son las que solo incluyen desplazamientos y rotaciones. Las matrices de coeficientes tienen la siguiente estructura:\n",
    "\n",
    "\n",
    "$$ D(d_x,d_y) \\equiv\n",
    "  \\begin{bmatrix}1 & 0 & d_x\\\\\n",
    "                 0 & 1 & d_y\\\\\n",
    "                 0 & 0 & 1  \\end{bmatrix}\\quad\n",
    "  \\quad\n",
    "  R(\\alpha)\\equiv\\begin{bmatrix}\\cos(\\alpha) & -\\sin(\\alpha) & 0\\\\\n",
    "                 \\sin(\\alpha) & \\cos(\\alpha) & 0\\\\\n",
    "                 0 & 0 & 1  \\end{bmatrix}$$\n",
    "                 \n",
    "Dentro de un momento veremos que consiguen el efecto deseado al operar sobre las coordenadas homogéneas de los puntos.\n",
    "\n",
    "\n",
    "Las transformaciones **similares** incluyen también escalados uniformes:\n",
    "\n",
    "$$\n",
    "S(s)\\equiv \\begin{bmatrix}s & 0 & 0\\\\\n",
    "                 0 & s & 0\\\\\n",
    "                 0 & 0 & 1  \\end{bmatrix}$$\n",
    "\n",
    "Este tipo de transformación (a veces se llaman también *métrica*) preserva completamente la forma de los objetos. En realidad solo cambiamos el sistema de referencia.\n",
    "\n",
    "\n",
    "Las transformaciones **afines** admiten también escalados no uniformes. \n",
    "\n",
    "$$\n",
    "S(s_x,s_y)\\equiv\\begin{bmatrix}  s_x & 0 & 0\\\\\n",
    "                 0 & s_y & 0\\\\\n",
    "                 0 & 0 & 1  \\end{bmatrix}$$\n",
    "\n",
    "Al combinar este efecto con rotaciones se puede estirar o comprimir el objeto en cualquier dirección.\n",
    "\n",
    "La composición de transformaciones equivale al producto de matrices: cuando se efectúa una secuencia de transformaciones, el resultado es equivalente a aplicar una única matriz que es el producto de todas ellas.\n",
    "\n",
    "$$T_3 (T_2 (T_1 x)) = (T_3T_2T_1)x$$\n",
    "\n",
    "El grupo afín tiene la estructura siguiente:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}  x & x & x\\\\\n",
    "                 x & x & x\\\\\n",
    "                 0 & 0 & x  \\end{bmatrix}$$\n",
    "                 \n",
    "Esto quiere decir que cuando se combinan desplazamientos, rotaciones y escalados no uniformes la matriz resultante o equivalente tiene dos ceros en la tercera fila, y el resto de los elementos puede ser cualquiera. La tercera coordenada homogénea solo se utiliza para \"implementar\" los desplazamientos. Una transformación afín es una operación esencialmente lineal sobre las coordenadas cartesianas (multiplicar por constantes y sumar). Veremos que puede ser útil como aproximación, pero no sirve para modelar la perspectiva, que es esencialmente no lineal (involucra una división de coordenadas). \n",
    "\n",
    "\n",
    "\n",
    "En las transformaciones **proyectivas** aparecen los 9 elementos.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}  x & x & x\\\\\n",
    "                 x & x & x\\\\\n",
    "                 x & x & x  \\end{bmatrix}$$\n",
    "                 \n",
    "Estas sí incluyen las transformaciones de perspectiva. (Cuando estudiemos el modelo de cámara en el capítulo siguiente  veremos que la clave para implementar la división de coordenadas es \"no hacerlo\": se deja para el final, cuando recuperamos las coordenadas cartesianas para dibujar. Mientras tanto podemos continuar operarando de forma lineal.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invariantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada grupo de transformaciones mantiene invariantes ciertas propiedades geométricas. En realidad, son estos invariantes los que realmente caracterizan a cada grupo.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Las transformaciones euclídeas matienen las **distancias** entre puntos. Las puertas, ventanas, el tejado de la casita del dibujo de arriba son del mismo tamaño.\n",
    "\n",
    "Las transformaciones similares mantienen los **ángulos** (y los ratios de distancias). Los ángulos rectos, los ángulos del tejado, etc. son iguales.\n",
    "\n",
    "Las transformaciones afines mantienen el **paralelismo** (y proporciones de áreas, y proporciones de distancias en la misma dirección). Los lados opuestos de la casa, puertas y ventanas, se mantienen paralelos, aunque los ángulos se deforman. Pero el área de la puerta dividida por la del tejado se mantiene constante. Se conserva bastante estructura.\n",
    "\n",
    "Las transformaciones proyectivas mantienen las **líneas rectas**. Se pierde el paralelismo, los ángulos, las distancias, proporciones de áreas y de distancias... Pero sigue quedando suficiente estructura para poder reconstruir la realidad.\n",
    "Por ejemplo, se preserva el **cross-ratio** de cuatro puntos en una recta, que se define como muestra la figura:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/demos/cross-ratio.png\"/>\n",
    "\n",
    "$$\\rho= \\frac{a}{a+b} \\; \\frac{c}{c+b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las transformaciones de *perspectiva* que observamos en las imágenes de un plano tomadas con una cámara son un caso especial de las proyectivas.\n",
    "\n",
    "Si vemos cuatro puntos alineados el valor de cross-ratio $\\rho$ será el mismo en el mundo real y en cualquier imagen. Da igual la posición de la cámara y el zoom elegido.\n",
    "\n",
    "En principio este invariante se puede utilizar para reconocer objetos, pero es numéricamente muy impreciso. En la actualidad se recomiendan otros métodos.\n",
    "\n",
    "Nota: Todos los resultados de este tema y los siguientes suponen que la cámara no tiene distorsión radial. Puedes comprobarlo fácilmente viendo si las líneas rectas se curvan cerca de los bordes de la imagen. Las cámaras modernas tienen ópticas bastante buenas, por lo que la distorsión radial es inapreciable a menos que se trabaje con campos de visión muy amplios.\n",
    "\n",
    "\n",
    "\n",
    "**Experimento**: Comprueba en dos imágenes de una escena en la que puedas identificar 4 puntos alineados que su cross-ratio es aproximadamente igual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente imagen muestra el típico efecto de perspectiva.\n",
    "\n",
    "![poles](../images/poles2.jpg)\n",
    "\n",
    "Suponiendo que los postes telefónicos están situados a la misma distancia ($a=b=c$), el cross ratio de cada 4 de ellos será $\\rho=1/4$. \n",
    "\n",
    "Entonces, si marcamos la base de 3 de ellos consecutivos, podemos deducir la posición del siguiente. Y así sucesivamente, podemos predecir la posición de todos los que queramos de la secuencia. En el ejemplo siguiente se han marcado con el ratón los tres primeros puntos, etiquetados con 0, 1, 2. Los tres puntos siguientes se han deducido mediante este procedimiento. \n",
    "\n",
    "![poles](../images/demos/cr2.png)\n",
    "\n",
    "Además, podemos deducir directamente el punto de fuga (el cuarto punto rojo), con el cross-ratio de 3 puntos equiespaciados y el cuarto en el infinito ($a=b,\\; c\\rightarrow\\infty$).\n",
    "\n",
    "Lo mismo con la parte superior de los postes:\n",
    "\n",
    "![poles](../images/demos/cr1.png)\n",
    "\n",
    "Ojo: la posición estimada es muy sensible al punto donde marcamos con el ratón.\n",
    "\n",
    "**Ejercicio CR**: reproduce este experimento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Herramientas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2   as cv\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive\n",
    "\n",
    "def fig(w,h):\n",
    "    plt.figure(figsize=(w,h))\n",
    "\n",
    "def readrgb(file):\n",
    "    return cv.cvtColor( cv.imread('../images/'+file), cv.COLOR_BGR2RGB) \n",
    "\n",
    "def rgb2gray(x):\n",
    "    return cv.cvtColor(x,cv.COLOR_RGB2GRAY)\n",
    "\n",
    "def imshowg(x):\n",
    "    plt.imshow(x, \"gray\")\n",
    "\n",
    "# para imprimir arrays con el número de decimales deseados\n",
    "import contextlib\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def printoptions(*args, **kwargs):\n",
    "    original = np.get_printoptions()\n",
    "    np.set_printoptions(*args, **kwargs)\n",
    "    yield \n",
    "    np.set_printoptions(**original)\n",
    "\n",
    "# imprime un array con pocos decimales\n",
    "def sharr(a, prec=3):\n",
    "    with printoptions(precision=prec, suppress=True):\n",
    "        print(a)\n",
    "\n",
    "# dibuja un polígono cuyos nodos son las filas de un array 2D\n",
    "def shcont(c, color='blue', nodes=True):\n",
    "    x = c[:,0]\n",
    "    y = c[:,1]\n",
    "    x = np.append(x,x[0])\n",
    "    y = np.append(y,y[0])\n",
    "    plt.plot(x,y,color)\n",
    "    if nodes: plt.plot(x,y,'.',color=color, markersize=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crea un vector (array 1D)\n",
    "# (no es imprescindible, numpy admite tuplas o listas en muchas funciones,\n",
    "# pero a esta función es útil a veces para aplicar operaciones aritméticas)\n",
    "def vec(*argn):\n",
    "    return np.array(argn)\n",
    "\n",
    "# convierte un conjunto de puntos ordinarios (almacenados como filas de la matriz de entrada)\n",
    "# en coordenas homogéneas (añadimos una columna de 1)\n",
    "def homog(x):\n",
    "    ax = np.array(x)\n",
    "    uc = np.ones(ax.shape[:-1]+(1,))\n",
    "    return np.append(ax,uc,axis=-1)\n",
    "\n",
    "# convierte en coordenadas tradicionales\n",
    "def inhomog(x):\n",
    "    ax = np.array(x)\n",
    "    return ax[..., :-1] / ax[...,[-1]]\n",
    "\n",
    "# estas dos funciones funcionan en espacios de cualquier dimensión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de umucv `htrans` aplica una homografía H a un conjunto de puntos almacenados como filas de una matriz.\n",
    "\n",
    "Internamente convierte a coordenadas homogéneas, se multiplican todos los puntos, y se convierte el resultado a coordenadas cartesianas. La costumbre de almacenar los puntos por filas hace que haya que multiplicar a la derecha por matriz traspuesta. Si estuvieran almacenados por columnas se multiplicaría directamente por la izquierda: $(Hx)^\\mathsf T = x^\\mathsf T H^\\mathsf T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aplica una transformación homogénea h a un conjunto\n",
    "# de puntos ordinarios, almacenados como filas\n",
    "def htrans(h,x):\n",
    "    return inhomog(homog(x) @ h.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos las matrices homogéneas de desplazamientos, rotaciones y cambios de escala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desp(d):\n",
    "    dx,dy = d\n",
    "    return np.array([\n",
    "            [1,0,dx],\n",
    "            [0,1,dy],\n",
    "            [0,0,1]])\n",
    "\n",
    "def scale(s):\n",
    "    sx,sy = s\n",
    "    return np.array([\n",
    "            [sx,0,0],\n",
    "            [0,sy,0],\n",
    "            [0,0,1]])\n",
    "\n",
    "# rotación eje \"vertical\" del plano\n",
    "def rot3(a):\n",
    "    c = np.cos(a)\n",
    "    s = np.sin(a)\n",
    "    return np.array([\n",
    "            [c,-s,0],\n",
    "            [s, c,0],\n",
    "            [0, 0,1]])\n",
    "\n",
    "pi = np.pi\n",
    "degree = pi/180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una figura de referencia para ver el resultado de diferentes transformaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = np.array(\n",
    "   [[0,   0  ],\n",
    "    [0,   1  ],\n",
    "    [0.5, 1  ],\n",
    "    [0.5, 0.5],\n",
    "    [1,   0.5],\n",
    "    [1,   0  ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig(4,4); \n",
    "shcont(ref); plt.axis([-2,2,-2,2]); plt.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar interactivamente el efecto de un desplazamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cada vez que movemos el trackbar\n",
    "def fun(a = 0):\n",
    "    # construimos la transformación con el nuevo parámetro\n",
    "    H = desp((a,0))\n",
    "    #print(H)\n",
    "\n",
    "    # transformamos los puntos de la referencia\n",
    "    img = htrans(H,ref)\n",
    "    #print(img)\n",
    "\n",
    "    fig(8,4)\n",
    "    # dibujamos el objeto de partida\n",
    "    plt.subplot(1,2,1)\n",
    "    shcont(ref)\n",
    "    plt.axis([-2,2,-2,2]); plt.grid()\n",
    "\n",
    "    # dibujamos el objeto transformado\n",
    "    plt.subplot(1,2,2)\n",
    "    shcont(img,color='red')\n",
    "    plt.axis([-2,2,-2,2]); plt.grid()\n",
    "\n",
    "interactive(fun, a=(-1,1,0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalmente el plano transformado es distinto del inicial, aunque para comparar el efecto de algunas transformaciones podemos dibujarlas juntas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(a = 0.2):\n",
    "    H = desp((a,0))\n",
    "    img = htrans(H,ref)\n",
    "\n",
    "    fig(4,4)\n",
    "    shcont(ref)\n",
    "    shcont(img,color='red')\n",
    "    plt.axis([-2,2,-2,2]); plt.grid()\n",
    "\n",
    "interactive(fun, a=(-1,1,0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora comprobamos el efecto de un escalado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(a = 1.6):\n",
    "    H = scale((a,a))\n",
    "    #print(H)\n",
    "    img = htrans(H,ref)\n",
    "\n",
    "    fig(4,4)\n",
    "    shcont(ref)\n",
    "    shcont(img,color='red')\n",
    "    plt.axis([-2,2,-2,2]); plt.grid()\n",
    "\n",
    "interactive(fun, a=(0,2,0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa que el \"centro de expansión\" es el origen (0,0). \n",
    "Si queremos escalar desde otro punto tenemos que trasladar la imagen, escalar y devolverla a su posición inicial. Esto es un buen ejemplo de la composición de transformaciones simples para conseguir un efecto más complejo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(a = 1.6):\n",
    "    H = desp((0.5,0.5)) @ scale((a,a)) @ desp(-vec(0.5,0.5))\n",
    "    img = htrans(H,ref)\n",
    "\n",
    "    fig(4,4)\n",
    "    shcont(ref)\n",
    "    shcont(img,color='red')\n",
    "    plt.axis([-2,2,-2,2]); plt.grid()\n",
    "\n",
    "interactive(fun, a=(0,2,0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las transformaciones **similares** combinan desplazamientos, rotaciones y escalados uniformes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(s = 1.6, a = 30):\n",
    "    H = desp(-vec(1,1)/2) @ scale((s,s)) @ rot3(a*degree) @ desp(-vec(0.5,0.5))\n",
    "    img = htrans(H,ref)\n",
    "\n",
    "    fig(4,4)\n",
    "    shcont(ref)\n",
    "    shcont(img,color='red')\n",
    "    plt.axis([-2,2,-2,2]); plt.grid()\n",
    "\n",
    "interactive(fun, s=(0,2,0.1), a=(-180,180,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa que estamos rotando la figura respecto a un punto que no es el origen. Lo conseguimos de nuevo con desplazamientos del objeto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si realizamos un escalado no uniforme obtenemos una **transformación afín**, que permite aproximar las vistas en *perspectiva débil* (objetos alejados de la cámara)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(a = 40, s = 1.4, sr = 0.4):\n",
    "    H   = desp((0.5,0.5)) @ scale((s,s*sr)) @ rot3(a*degree)  @ desp(-vec(0.5,0.5))\n",
    "    print(H)\n",
    "    img = htrans(H,ref)\n",
    "\n",
    "    fig(4,4)\n",
    "    shcont(ref)\n",
    "    shcont(img,color='red')\n",
    "    plt.axis([-2,2,-2,2]); plt.grid()\n",
    "\n",
    "interactive(fun, a=(-180,180,10), s=(0,2,0.2), sr=(0,2,0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este tipo de aproximación es útil en algunas aplicaciones pero no es muy realista: no tiene puntos de fuga de las rectas paralelas.\n",
    "\n",
    "Finalmente, el efecto de perspectiva real se puede observar con la siguiente transformación (conseguida con un modelo de cámara que veremos el próximo día)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = np.array([[250,-11, 260],\n",
    "              [22,  33, 320],\n",
    "              [0.2,  0.3, 1]])\n",
    "\n",
    "img = htrans(H,ref)\n",
    "\n",
    "shcont(img)\n",
    "plt.ylim(480,0); plt.xlim(0,640); plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matriz de transformación es del grupo **proyectivo**, no tiene ninguna restricción en sus 9 elementos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimación de transformaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un problema clave de la geometría visual es encontrar la transformación proyectiva que relaciona dos conjuntos de puntos. Esto implica resolver un [sistema de ecuaciones](sistecs.ipynb). (Como veremos [más adelante](DLT.ipynb), es posible escribir una función sencilla que resuelve este problema.) En la práctica es recomendable usar la función de OpenCV `findHomography`. (La palabra *homography* significa \"transformación proyectiva\" (transformación lineal de coordenadas homogéneas $\\simeq$ matriz $3\\times 3$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos que  `findHomography` funciona bien con los datos del ejemplo artificial anterior. Sabemos que los puntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "son el resultado de aplicar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharr(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a los puntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efectivamente, recuperamos la transformación a partir de los puntos correspondientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "He, mask = cv.findHomography(ref, img)\n",
    "\n",
    "sharr(He)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También devuelve un array \"mask\" indicando las correspondencias que se consideran correctas, ya que `findHomography` puede, opcionalmente, aplicar técnicas de estimación robusta capaces de tolerar correspondencias erróneas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una transformación proyectiva queda completamente determinada cuando sabemos cómo actúa sobre 4 puntos.\n",
    "Tiene 8 grados de libertad (la matriz $3\\times3$ tiene 9 coeficientes pero el factor de escala global es arbitrario). Como cada punto del plano tiene 2 grados de libertad, los 8 grados de libertad de la transformación quedan fijados por 4 puntos.\n",
    "\n",
    "Si queremos estimar una transformación proyectiva a partir del número mínimo de 4 puntos, podemos usar la función más simple `cv.getPerspectiveTransform`. Si hay más, tenemos información redundante pero que en la práctica es útil para mejorar la estimación a partir de datos con ruido. En este caso usamos `cv.findHomography`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rectificación de planos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si descubrimos en una imagen plana 4 ó más puntos cuyas posiciones reales son conocidas podemos obtener la homografía que se ha producido y \"deshacerla\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El análogo a `htrans` para imágenes completas es `cv.warpPerspective`. Podemos probar algunas transformaciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = readrgb('disk1.jpg')\n",
    "\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = cv.warpPerspective(\n",
    "    img,                                # imagen de entrada\n",
    "    desp((100,100)) @ rot3(20*degree),  # homografía (matriz 3x3)\n",
    "    (600,600))                          # tamaño de la imagen resultante\n",
    "\n",
    "plt.imshow(rec);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos interesa la homografía que convierte los vértices del tablero de ajedrez en un cuadrado. Cuando la apliquemos rectificará toda la imagen: arrastrará a todos los demás puntos a su posición correcta.\n",
    "\n",
    "Para no complicar ahora mucho el ejemplo encontramos manualmente por prueba y error las coordenadas aproximadas de las esquinas del tablero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view = np.array([\n",
    "        [140,280],\n",
    "        [40,140],\n",
    "        [225,100],\n",
    "        [350,200]])\n",
    "\n",
    "plt.imshow(img);\n",
    "shcont(view);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a transformar las cuatro esquinas observadas en un cuadrado. Elegimos las siguientes coordenadas para que el tablero recitificado tenga 100 pixels de lado, por ejemplo, y quede bien situado en la imagen final (por prueba y error),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = np.array([\n",
    "    [  50.,   80.],\n",
    "    [ 150.,   80.],\n",
    "    [ 150.,  180.],\n",
    "    [  50.,  180.]])\n",
    "\n",
    "fig(4,5); shcont(real); plt.axis([0,400,500,0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H,_ = cv.findHomography(view, real)\n",
    "\n",
    "sharr(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = cv.warpPerspective(img,H,(400,500))\n",
    "\n",
    "fig(6,8)\n",
    "plt.imshow(rec);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos ajustado manualmente el desplazamiento, escalado y tamaño de imagen destino para que quede bien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La transformación que rectifica los 4 puntos de referencia corrige todo el plano del suelo. El disco se vuelve circular y las baldosas recuperan su forma cuadrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los objetos que no están en ese plano quedan completamente deformados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La imagen rectificada pierde precisión a medida que nos alejamos de los puntos de referencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El rectángulo visible de la imagen inicial se convierte en un cuadrilátero irregular en la imagen rectificada. La zona negra exterior queda indefinida. Se puede recortar la zona de interés, o elegir la homografía y el tamaño del destino para que el resultado quede completamente relleno. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `cv.warpPerspective` tiene un segundo modo de funcionamiento en el que el resultado de la transformación se añade encima de una imagen de destino existente. En este caso se utiliza la opción de borde transparente para que la zona negra exterior no machaque el resto de la imagen destino. (Se muestra un ejemplo de uso en la última sección de este notebook \"Warping con borde transparente\".)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos estimado directamente la transformación de rectificación (imagen $\\rightarrow$ referencia). También se puede estimar la transformación de perspectiva sufrida (referencia $\\rightarrow$ imagen) y aplicar su inversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H,_ = cv.findHomography(real, view)\n",
    "H = np.linalg.inv(H)\n",
    "sharr(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No es la misma matriz (tiene un factor de escala global distinto) pero es la misma transformación. Si ajustamos la escala poniendo un uno en el mismo elemento comprobamos que los demás elementos son iguales (salvo errores de redondeo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharr(H/H[2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importante**: Los puntos de referencia no tienen que formar necesariamente un cuadrado. Simplemente necesitamos conocer las posiciones relativas de 4 o más puntos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En definitiva, con una homografía se pueden mover 4 puntos cualesquiera a las posiciones que deseemos. El resto de la imagen es arrastrada de forma coherente con ellos. Por tanto, es fácil programar numerosos efectos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para facilitar al máximo la detección de las coordenadas de un objeto conocido podemos aprovechar el paquete `pyzbar`, que nos devuelve las esquinas de los códigos QR (ejemplo `code/zbardemo.py`). Por ejemplo, podemos reproducir un vídeo en el código QR:\n",
    "\n",
    "![qrvirt1](../images/demos/qrvirt1.png)\n",
    "\n",
    "![qurvirt2](../images/demos/qrvirt2.png)\n",
    "\n",
    "Para conseguir este efecto hay que utilizar el segundo modo de funcionamiento de `cv.warpPersepctive` con borde transparente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro efecto divertido es el intercambio de cuadriláteros en una escena. Podemos cambiar los cuadros de sitio sin descolgarlos de la pared:\n",
    "\n",
    "![noflipped](../images/demos/noflipped.png)\n",
    "\n",
    "![noflipped](../images/demos/flipped.png)\n",
    "\n",
    "En una escena en la que se vean dos pantallas de ordenador podemos intercambiar lo que se muestra en cada una.\n",
    "\n",
    "OJO: no es trivial hacer esto automáticamente, es difícil detectar los cuadriáteros de interés de forma robusta.\n",
    "\n",
    "**Ejercicio SWAP**: Escribe una función `copy_quad(p,src,q,dst)` que mueve un cuadrilátero `p` (los cuatro puntos almacenados en un array $4\\times2$) de una imagen `src` a un cuadrilátero `q` en la imagen `dst`. (Pista: necesitarás un máscara y es conveniente reducir la operación al \"bounding box\" de las regiones.) Utilízala para reproducir el experimento anterior marcando los puntos de referencia manualmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volviendo a la rectificación de planos, si tenemos la homografía de rectificación y conocemos el tamaño real del objeto de referencia, podemos deducir las medidas reales de los objetos situados en el plano. Lo único que necesitamos es conocer el factor de escala que relaciona las dimensiones de la imagen rectificada y el objeto real.\n",
    "\n",
    "Un experimento interesante consiste en construir un medidor de distancias que se apoya en la detección de un DNI o tarjeta bancaria. Es un objeto plano de referencia de tamaño estándar (85mm$\\times$54mm) que siempre tenemos a mano. Por ejemplo, podemos usar el carnet de la UMU:\n",
    "\n",
    "![rectif](../images/demos/rectif/dni.png)\n",
    "\n",
    "Suponiendo que podemos estimar las esquinas, aplicamos el procedimiento anterior para rectificar el plano de la mesa. El rectángulo destino debe tener la proporción correcta. Si no la tiene, \n",
    "o nos equivocamos en la asociación de puntos (asociando el lado corto con el lado largo), la imagen rectificada estará mal:\n",
    "\n",
    "![rectif](../images/demos/rectif/dni5.png)\n",
    "\n",
    "Cuando la asociación de puntos es correcta la imagen rectificada recupera las proporciones correctas: por ejemplo, la pila de botón se vuelve circular. Observa también que los planos paralelos a la mesa también ser rectifican correctamente (la cara superior del cubo de Rubick) pero en ellos cambia la escala.\n",
    "\n",
    "\n",
    "![rectif](../images/demos/rectif/dni3.png)\n",
    "\n",
    "\n",
    "El factor de escala que he elegido es 1 pixel por milímetro. Marcando dos puntos en la imagen rectificada podemos deducir inmediatamente la distancia real entre ellos.\n",
    "\n",
    "![rectif](../images/demos/rectif/dni6.png)\n",
    "\n",
    "Pero también se pueden marcar los puntos en la imagen original y transformarlos:\n",
    "\n",
    "![rectif](../images/demos/rectif/dni4.png)\n",
    "\n",
    "En realidad el cubo mide 5cm de lado. Las medidas que conseguimos con este método son aproximadas. El error aumenta al alejarnos de la referencia. Hay varias fuentes de incertidumbre, entre las que se incluye la precisión de los puntos marcados y los de referencia.\n",
    "\n",
    "Si nos mantenemos muy cerca del plano de la mesa las medidas son aproximadameante correctas:\n",
    "\n",
    "![rectif](../images/demos/rectif/dni1.png)\n",
    "\n",
    "Pero fuera del plano los resultados no son válidos:\n",
    "\n",
    "![rectif](../images/demos/rectif/dni2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio RECTIF (a)**: Reproduce el experimento anterior y mide la longitud de la cuchara. Mide la distancia entre las monedas en `coins.png`.\n",
    "\n",
    "**Ejercicio**: Detecta automáticamente el carnet y estima con precisión las esquinas del rectángulo.\n",
    "\n",
    "En esta escena la detección del carnet es sencilla porque contrasta bastante bien con el color de la mesa. Se puede umbralizar automáticamente con el método de OTSU.\n",
    "\n",
    "Pero el principal problema que se plantea aquí es que las esquinas están redondeadas. La estimación automática de los vértices del rectángulo pasa por analizar el contorno para encontrar tramos \"muy rectos\", a partir de los cuales se puede deducir la posición de los vértices con las operaciones geométricas que vimos en la clase anterior.\n",
    "\n",
    "**Ejercicio CARD**: Sustituye la foto de un carnet en tiempo real.\n",
    "\n",
    "<video controls src=\"https://github.com/albertoruiz/umucv/raw/refs/heads/master/images/demos/dni.mp4\" style=\"border:0\"></video> \n",
    "\n",
    "\n",
    "Una vez que tenemos la homografía de rectificación podemos dibujar el aspecto que tendrían otros objetos planos si se vieran en la imagen. Los transformamos con la homografía inversa. Se consigue una realidad aumentada plana. (En el capítulo siguiente aprenderemos a situar objetos virtuales 3D en la escena). Por ejemplo, podemos mostrar el sistema de coordenadas definido por el carnet mediante una malla con cuadritos de 1cm$^2$.\n",
    "\n",
    "![grid](../images/demos/rectif/grid.png)\n",
    "\n",
    "\n",
    "**Ejercicio SUDOKU**: Rellena un sudoku con realidad aumentada plana en tiempo real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuerda que los puntos de referencia no tienen por qué formar un rectángulo. Y pueden ser más de cuatro, para aumentar la precisión. De hecho, los marcadores rectangulares tienen un problema de ambigüedad. Un marcador asimétrico como el siguiente define perfectamente el sistema de coordenadas del plano (y como veremos en el capítulo siguiente, también del espacio 3D), y además tiene una redundancia que permite descartar polígonos de 6 lados que no tengan la forma del marcador. Veremos la forma de detectar automáticamente este marcador en la clase de prácticas.\n",
    "\n",
    "![marcador](../images/ref.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra posibilidad es utilizar un marcador como el siguiente, compuesto de 4 círculos situados en posiciones conocidas. Los círculos se proyectan en la imagen como elipses que pueden detectarse sin mucha dificultad, como veremos en la sesión práctica.\n",
    "\n",
    "<img src=\"../images/elipses/circles1.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En otras ocasiones no hay ningún marcador artificial en la escena, pero con un poco de suerte se pueden encontrar puntos de referencia.\n",
    "\n",
    "**Ejercicio RECTIF (b)**: Determina la distancia a la portería a la que se encuentra el jugador que está tirando a puerta. \n",
    "\n",
    "![eder](../images/gol-eder.png)\n",
    "\n",
    "\n",
    "\n",
    "Debes elegir puntos de referencia visibles en la escena que tengan posiciones fijas en un campo de fútbol (algunas dimensiones del campo son variables).\n",
    "\n",
    "Esto no se puede automatizar, hay que marcar los puntos de referencia a mano. Para que sirva en otras fotos almacena en un archivo de texto que se pasa como parámetro las coordenadas reales de los puntos elegidos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones afines\n",
    "\n",
    "Las transformaciones afines (que incluyen desplazamientos, rotaciones y escalados diferentes en cada eje) tienen 6 grados de libertad. Quedan definidas por la transformación sufrida por 3 puntos. La matriz de transformación se obtiene con la función `cv.getAffineTransform`.\n",
    "\n",
    "Nota 1: Esta función (igual que cv.getPerspectiveTransform) requiere datos de tipo np.float32.\n",
    "\n",
    "Nota 2: La función devuelve una matriz $2x3$. Se sobreentiende que la última fila es $(0,0,1)$. Esta matriz se puede aplicar directamente con `cv.warpAffine`.\n",
    "\n",
    "**Experimento**: Las caras detectadas con los face_landmarks del paquete DLIB proporcionan 3 puntos de referencia (ojos y parte superior de la boca) razonablemente estables, que no dependen de la expresión de la cara. Si los llevamos a unas coordenadas fijas conseguimos una normalización afín de las caras:\n",
    "\n",
    "![faces-normal](../images/demos/faces-normal.png)\n",
    "\n",
    "(La máquina de deep learning *FaceNet* para reconocimiento de caras utiliza esta normalización como preproceso para entrenar el descriptor.)\n",
    "\n",
    "**Ejercicio  NFACE**: Reproduce la imagen anterior. Coloca todas las caras detectadas en la imagen de entrada ordenadas en una o varias filas, con los ojos y la boca en posiciones fijas.\n",
    "\n",
    "**Ejercicio**: Escribe una función `move_tri(p,src,q,dst)` que transfiere el triángulo definido por los puntos `p` (tres puntos almacenados en un array $3\\times2$) de la imagen `src` al triángulo `q` de la imagen destino."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mosaico de imágenes\n",
    "\n",
    "Las vistas de un plano desde diferentes puntos de vista están relacionadas por una homografía, como acabamos de ver.\n",
    "\n",
    "Se puede demostrar que las imágenes del mundo 3D tomadas desde la misma posición también se relacionan mediante homografías planas. Esto nos permite hacer un mosaico, \"pegando\" todas en un marco de referencia común.\n",
    "\n",
    "Recuerda: para hacer mosaicos, si la escena es 3D la cámara puede rotar o hacer zoom, pero no puede desplazarse. Si la escena es plana, las imágenes se pueden tomar desde cualquier posición."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a intentar encontrar automáticamente las homografías utilizando puntos de interés y sus descriptores como vimos en una clase anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb1 = readrgb('pano/pano001.jpg')\n",
    "rgb2 = readrgb('pano/pano002.jpg')\n",
    "\n",
    "x1 = rgb2gray(rgb1)\n",
    "x2 = rgb2gray(rgb2)\n",
    "\n",
    "fig(12,4)\n",
    "plt.subplot(1,2,1); imshowg(x2);\n",
    "plt.subplot(1,2,2); imshowg(x1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv.SIFT_create()\n",
    "(kps, descs) = sift.detectAndCompute(x1, None)\n",
    "print(\"# kps: {}, descriptors: {}\".format(len(kps), descs.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow( cv.drawKeypoints(image=x1,\n",
    "                             outImage=None,\n",
    "                             keypoints=kps,\n",
    "                             flags=4, color = (128,0,0)) );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(kps2, descs2) = sift.detectAndCompute(x2, None)\n",
    "descs2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow( cv.drawKeypoints(image=x2,\n",
    "                             outImage=None,\n",
    "                             keypoints=kps2,\n",
    "                             flags=4, color = (128,0,0)) );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf = cv.BFMatcher()\n",
    "\n",
    "matches = bf.knnMatch(descs2,descs,k=2)\n",
    "\n",
    "len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para quedarnos solo los que tienen muy poca ambigüedad.\n",
    "\n",
    "# Apply ratio test\n",
    "good = []\n",
    "for mt in matches:\n",
    "    if len(mt) == 2:\n",
    "        best, second = mt\n",
    "        if best.distance < 0.75*second.distance:\n",
    "            good.append(best) \n",
    "\n",
    "print(len(good))\n",
    "\n",
    "# drawMatches usa la estructura matches1to2\n",
    "img3 = cv.drawMatches(x2,kps2,\n",
    "                      x1,kps,\n",
    "                      good,\n",
    "                      flags=2,outImg=None,\n",
    "                      matchColor=(128,0,0))\n",
    "fig(12,4)\n",
    "plt.imshow(img3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se han encontrado muchas correspondencias pero algunas son incorrectas. Necesitaremos una técnica de estimación robusta como [RANSAC](https://en.wikipedia.org/wiki/Random_sample_consensus). (\"*random sample consensus*\", que prueba muchas hipótesis y se queda con la que tiene más correspondencias compatibles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a partir de los matchings seleccionados construimos los arrays de puntos que necesita findHomography\n",
    "src_pts = np.array([ kps [m.trainIdx].pt for m in good ]).astype(np.float32).reshape(-1,2)\n",
    "dst_pts = np.array([ kps2[m.queryIdx].pt for m in good ]).astype(np.float32).reshape(-1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H, mask = cv.findHomography(src_pts, dst_pts, cv.RANSAC, 3) # cv.LMEDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharr(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por curiosidad podemos ver las correspondencias que el método RANSAC considera correctas (aunque no hacen falta para nada más)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask viene como una array 2D de 0 ó 1, lo convertimos a un array 1D de bool\n",
    "matchesMask = mask.ravel()>0\n",
    "\n",
    "ok = [ good[k] for k in range(len(good)) if matchesMask[k] ]\n",
    "\n",
    "# good es una lista, si fuera un array prodríamos hacer directamente\n",
    "# ok = good[matchesMask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img4 = cv.drawMatches(x2,kps2,x1,kps,ok,flags=2,outImg=None,matchColor=(0,255,0))\n",
    "\n",
    "fig(12,4)\n",
    "plt.imshow(img4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El paso siguiente es llevar todas las imágenes a un marco común con espacio suficiente. Para ello podemos usar la siguiente función auxiliar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t(h,x):\n",
    "    return cv.warpPerspective(x, desp((100,150)) @ h,(1000,600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La imagen \"base\" queda así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig(15,8)\n",
    "\n",
    "imshowg( t(np.eye(3),x2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y la otra queda transformada para coincidir en la zona común:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig(15,8)\n",
    "\n",
    "imshowg( t(H,x1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una forma rápida de combinar las dos es tomar el máximo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig(15,8)\n",
    "\n",
    "imshowg( np.maximum( t(np.eye(3),x2), t(H,x1) ) );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También funciona en color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig(15,8)\n",
    "\n",
    "plt.imshow( np.maximum( t(np.eye(3),rgb2) , t(H,rgb1) ) );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la colección de imágenes tenemos 7 vistas (pano/pano001.jpg ... pano/pano007.jpg) que pueden usarse para hacer un mosaico más grande. Pero ¡ojo! cuando el panorama abarca un ángulo grande la distorsión se hace enorme y es necesario recurrir a una proyección cilíndrica o esférica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Warping* con borde transparente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una alternativa al método anterior de unión de imágenes consiste realizar la transformación directamente sobre la una imagen existente en lugar de crear una nueva. En este caso hay que poner borde transparente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilidad para devolver el número de correspondencias y la homografía entre dos imágenes\n",
    "\n",
    "sift = cv.SIFT_create()\n",
    "bf = cv.BFMatcher()\n",
    "\n",
    "def match(query, model):\n",
    "    x1 = query\n",
    "    x2 = model\n",
    "    (k1, d1) = sift.detectAndCompute(x1, None)\n",
    "    (k2, d2) = sift.detectAndCompute(x2, None)\n",
    "\n",
    "    matches = bf.knnMatch(d1,d2,k=2)\n",
    "    # ratio test\n",
    "    good = []\n",
    "    for m in matches:\n",
    "        if len(m) == 2:\n",
    "            best, second = m\n",
    "            if best.distance < 0.75*second.distance:\n",
    "                good.append(best)\n",
    "\n",
    "    if len(good) < 6: return 6, None\n",
    "\n",
    "    src_pts = np.array([ k2[m.trainIdx].pt for m in good ]).astype(np.float32).reshape(-1,2)\n",
    "    dst_pts = np.array([ k1[m.queryIdx].pt for m in good ]).astype(np.float32).reshape(-1,2)\n",
    "\n",
    "    H, mask = cv.findHomography(src_pts, dst_pts, cv.RANSAC, 3)\n",
    "\n",
    "    return sum(mask.flatten()>0), H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "pano = [readrgb(x) for x in sorted(glob.glob('../images/pano/pano*.jpg'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(match(pano[1],pano[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos todas las parejas y las ordenamos por número de correspondencias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([(match(p,q)[0],i,j) for i,p in enumerate(pano) for j,q in enumerate(pano) if i< j],reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elegimos como base una de las que tienen más corresponencias.\n",
    "\n",
    "La transformación T sirve para desplazar la imagen al centro en el espacio más grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h,w,_ = pano[6].shape\n",
    "mw,mh = 400,100\n",
    "T = desp((mw,float(mh)))\n",
    "sz = (w+2*mw,h+2*mh)\n",
    "base = cv.warpPerspective(pano[6], T , sz)\n",
    "\n",
    "fig(10,6)\n",
    "plt.imshow(base);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos añadiendo las que tengan más correspondencias con alguna anterior. (Lo hacemos a ojo, mirando la lista anterior, aunque el objetivo final es automatizarlo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,H67 = match(pano[6],pano[7])\n",
    "cv.warpPerspective(pano[7],T@H67,sz, base, 0, cv.BORDER_TRANSPARENT)\n",
    "fig(10,6)\n",
    "plt.imshow(base);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,H65 = match(pano[6],pano[5])\n",
    "cv.warpPerspective(pano[5],T@H65,sz, base, 0, cv.BORDER_TRANSPARENT)\n",
    "fig(10,6)\n",
    "plt.imshow(base);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composición de homografías"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importante: la siguiente imagen que queremos añadir no tiene correspondencias con la base, pero podemos obtener la homografía mediante composición a través de una imagen intermedia: $H_{6 \\leftarrow 4} = H_{6 \\leftarrow 5} H_{5 \\leftarrow 4} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,H54 = match(pano[5],pano[4])\n",
    "cv.warpPerspective(pano[4],T@H65@H54,sz, base, 0, cv.BORDER_TRANSPARENT)\n",
    "fig(10,6)\n",
    "plt.imshow(base);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra posibilidad es calcular nuevos puntos de interés en la imagen base actual para buscar coincidencias con las restantes, pero la deformación puede reducir el número de correspondencias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitaciones de la proyección plana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos un poco más en detalle el problema de la deformación.\n",
    "\n",
    "Para construir un panorama los campos visuales de las imágenes deben estar solapados:\n",
    "\n",
    "<img src='../images/demos/pano/pano0.svg' width='20%'/>\n",
    "\n",
    "Las homografías transfieren las imágenes al plano de la imagen base:\n",
    "\n",
    "<img src='../images/demos/pano/pano1.svg' width='40%'/>\n",
    "\n",
    "Si el campo visual total es muy amplio, o elegimos mal la base, será imposible proyectar todas las imágenes en un plano:\n",
    "\n",
    "<img src='../images/demos/pano/pano2.svg' width='40%'/>\n",
    "\n",
    "La imagen izquierda no entra. Se extendería hasta el infinito (\"y más allá\", volviendo por el otro extremo: el punto del infinito en una dirección es el mismo en los dos sentidos).\n",
    "\n",
    "\n",
    "Cuando la base está en medio, las imágenes de los extremos se deforman poco:\n",
    "\n",
    "<img src='../images/demos/pano/base1.svg' width='40%'/>\n",
    "\n",
    "En la misma situación, si la base es la imagen derecha, la de la izquierda se deformará mucho más que antes:\n",
    "\n",
    "<img src='../images/demos/pano/base0.svg' width='40%'/>\n",
    "\n",
    "\n",
    "En un panorama muy amplio como el de este ejemplo hay que elegir con cuidado la base para no superar el ángulo máximo.\n",
    "\n",
    "<img src='../images/demos/pano/all.svg' width='80%'/>\n",
    "\n",
    "A pesar de todo la deformacion en los extremos es enorme:\n",
    "\n",
    "<img src='../images/demos/pano/all.png' width='100%'/>\n",
    "\n",
    "Se puede recortar la zona central, pero realmente este método solo es útil para panoramas de poca amplitud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV tiene una [clase](https://docs.opencv.org/3.4.1/d2/d8d/classcv_1_1Stitcher.html) para crear automáticamente panoramas. Su uso, muy simple, se ilustra en el ejemplo de código [`stitcher.py`](../code/stitcher.py). Aquí vemos el resultado que se consigue sobre nuestras imágenes y cómo la proyección esférica empleada permite abarcar un ángulo muy grande a costa de curvar las líneas rectas. Observa también la calidad del \"blending\" (las uniones).\n",
    "\n",
    "![stitched](../images/demos/stitched.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí podemos encontrar un documento interesante sobre la construcción de [panoramas](https://graphics.stanford.edu/courses/cs448a-10/kari-panoramas-02mar10-opt.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio PANO**. Crea automáticamente un mosaico a partir de las imágenes en una carpeta. Las imágenes no tienen por qué estar ordenadas ni formar una cadena lineal y no sabemos el espacio que ocupa el resultado. El usuario debe intervenir lo menos posible. Usa el método sencillo basado en homografías y compara con el stitcher de OpenCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escenas planas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los mosaicos de escenas planas usando homografías pueden ser en teoría tan amplios como queramos sin que se produzca distorsión.\n",
    "\n",
    "<img src=\"../images/demos/big.png\" width=\"60%\"/>\n",
    "\n",
    "En la práctica es necesario tener mucho solapamiento entre las imágenes para que los errores de estimación acumulados no sean excesivos.\n",
    "\n",
    "[Hiperresolución](https://www.rijksmuseum.nl/en/most-detailed-ever-photograph-of-the-night-watch-goes-online): 528 fotos para crear una composición que ocupa 44.8 Gigapixels."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

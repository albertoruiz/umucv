{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las redes convolucionales son especialmente adecuadas para procesar imágenes. \"Aprenden\" los coficientes de los filtros que son más adecuados para resolver cada problema concreto.\n",
    "\n",
    "En el capítulo anterior hemos comprobado su eficacia en el problema relativamente sencillo de reconocimiento de dígitos manuscritos. \n",
    "\n",
    "En este tema vamos a poner en marcha varios modelos ya preentrenados que resuelven problemas realistas y se han considerado como avances muy relevantes en visión artificial. (Aparte de esto, OpenCV tiene un [módulo de deep learning](https://docs.opencv.org/4.x/d2/d58/tutorial_table_of_content_dnn.html) para trabajar con modelos de pytorch, tensorflow y otros entornos.)\n",
    "\n",
    "(Si tienes una GPU con CUDA puedes comparar la diferencia en el tiempo de cómputo respecto al proceso en CPU.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tareas típicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos problemas típicos de la visión artificial que podemos atacar mediante *deep learning* son los siguientes. (Entre paréntesis se indican las subcarpetas de `code/DL` que contienen ejemplos de código relacionados. Los que tienen prefijo `mp_` están proporcionados por la colección de mediapipe.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Clasificación de la imagen completa (`inception`, `mp_classifier`)\n",
    "-  Detección de diferentes clases de objetos en la imagen, obteniendo sus *bounding boxes* y categorías (`yolo`, `mp_object`)\n",
    "-  Extracción de máscaras de segmentación de forma general (`mp_segmenter`, `mp_segmenter_point`, `SAM`, `UNET`)\n",
    "-  Detección de pose humana con *landmarks* (`mp_face`, `mp_hands`, `mp_humanpose`)\n",
    "-  *Universal features* preentrenadas que sirven como entrada para clasificadores ligeros (`mp_embedder`, `facerec`, `DINO`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imagenet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La base de datos [imagenet](http://www.image-net.org/) es un conjunto masivo de imágenes naturales pertenecientes a miles de categorías que se utilizó como benchmark en la competición para conseguir máquinas de vision artificial con capacidad próxima a la humana. Aquí hay un subconjunto con [imágenes reducidas](https://www.kaggle.com/c/tiny-imagenet).)\n",
    "\n",
    "El paquete keras incluye varias [máquinas preentrenadas](https://keras.io/applications) para resolver este problema. Como se observa en la tabla disponible en esa página, la precisión \"top 1\" no es espectacular (~78%), pero hay que tener en cuenta que el número de clases es enorme.\n",
    "\n",
    "El ejemplo de código [inception0.py](../code/DL/inception/inception0.py) muestra la forma de utilizar estas máquinas sobre la imagen en vivo. Se puede elegir entre varios modelos cambiando una variable global y es fácil añadir más. El que parece funcionar mejor de los que he probado es `inception_v3`. Se clasifica la región central marcada con un cuadrado, el resto de la imagen no se tiene en cuenta.\n",
    "\n",
    "![platanos](../images/demos/platanos.png)\n",
    "\n",
    "(Esta carpeta incluye también una variante `inception1.py` que hace una captura asíncrona. El objetivo es poner todo el GUI en un hilo y hacer un bucle de procesamiento aparte. Si varios hilos actualizan las ventanas el GUI de opencv se bloquea.)\n",
    "\n",
    "Al operar sobre imágenes del mundo real, estos modelos cometen algunos errores más o menos justificables, como se muestra en las imágenes de prueba incluidas en la carpeta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la sesión de prácticas pondremos en marcha un tipo especial de red convolucional que puede utilizarse para obtener máscaras de segmentación. Si el problema es sencillo no requiere muchas imágenes etiquetadas ni excesivo esfuerzo computacional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las redes convolucionales anteriores tienen una entrada de tamaño fijo. Para detectar objetos a diferentes escalas y posiciones es necesario ejecutar la máquina sobre múltiples regiones candidatas de la imagen, lo que resulta muy ineficiente.\n",
    "\n",
    "Un modelo mucho mejor es [YOLO](https://pjreddie.com/darknet/yolo/) (You Only Look Once). En vez de producir como salida un vector de probabilidades de las posibles clases, genera simultáneamente bounding boxes de los posibles objetos con sus probabilidades. Se hace una única ejecución de la red y las detecciones tienen en cuenta todo el contexto de la imagen.\n",
    "\n",
    "En la carpeta `code/DL/yolo`tenemos un script para utilizar la implementación de YOLO V11 proporcionada por el sistema [ultralytics](https://www.ultralytics.com/).\n",
    "\n",
    "\n",
    "![yolo](../images/demos/yolo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En un capítulo anterior hemos visto que el paquete DLIB proporciona un método de detección de caras bastante preciso basado en HOG (histograma de orientaciones de gradiente). Además, incluye un detector de marcadores faciales capaz de estimar la posición de los puntos clave de la cara. Estas posiciones son muy útiles para muchas aplicaciones pero son insuficientes para conseguir un reconocimiento de personas robusto.\n",
    "\n",
    "Una de las primeras formas de atacar el reconocimiento de caras fue la reducción de dimensión con PCA ([eigenfaces](https://en.wikipedia.org/wiki/Eigenface)). Esta idea puede mejorarse con una transformación no lineal obtenida con una red convolucional profunda con estructura de \"cuello de botella\". Un avance significativo se produjo con el método [FaceNet](https://www.cv-foundation.org/openaccess/content_cvpr_2015/app/1A_089.pdf), que se basa en **tripletas** de imágenes para construir un vector de 128 features optimizado para conseguir valores iguales con caras de la misma persona y diferentes para personas distintas. Para que esto funcione es necesario alinear muy bien las caras en un marco común, lo que se consigue con los marcadores faciales de DLIB. [Este blog](https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78) tiene una explicación excelente.\n",
    "\n",
    "El paquete [face recognition](https://github.com/ageitgey/face_recognition) está basado en [esta implementación](https://cmusatyalab.github.io/openface/) de FaceNet. En la carpeta `code/DL/facerec` tenemos un ejemplo de código con nuestro método general de captura con autoStream. Simplemente se compara el descriptor de los modelos con los de las caras detectadas y se clasifica por mínima distancia.\n",
    "\n",
    "El método funciona muy bien, como se puede comprobar en la prueba siguiente. Hemos cogido de internet las fotos de los componentes de Monty Python:\n",
    "\n",
    "![models](../images/demos/monty.png)\n",
    "\n",
    "Veamos si los reconoce con diferentes edades:\n",
    "\n",
    "![models](../images/demos/monty-rec1.png)\n",
    "\n",
    "En esta primera prueba, donde los personajes están muy caracterizados, no he conseguido encontrar un modelo de Terry Gilliam que lo reconociera. Pero el modelo joven lo reconoce en la siguiente foto, donde están más mayores:\n",
    "\n",
    "![models](../images/demos/monty-rec2.png)\n",
    "\n",
    "Los modelos mayores también reconocen a los personajes jóvenes:\n",
    "\n",
    "![models](../images/demos/monty-rec3.png)\n",
    "\n",
    "Pruébalo con la webcam o con tus fotos familiares para hacerte una idea de la precisión del método.\n",
    "\n",
    "Esta aplicación incluye muchas técnicas importantes utilizadas en visión artificial: un *encoding* (neuronal) obtenido del *alineamiento* (afín --lo estudiaremos la semana siguiente--) de los *landmarks* (basados en casacadas de regresores) de la cara, detectada con *HOG*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alineamiento afín *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de los face landmarks:\n",
    "\n",
    "![landmarks](../images/demos/face_landmarks.png)\n",
    "\n",
    "Detectamos 3 posiciones características:\n",
    "![align1](../images/demos/alignface1.png)\n",
    "\n",
    "Y transformamos las imágenes a un marco común:\n",
    "\n",
    "![align2](../images/demos/alignface2.png)\n",
    "\n",
    "También podemos intercambiarlas:\n",
    "\n",
    "![fake](../images/demos/fakefaces.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Body landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro problema muy complejo en el que el *deep learning* ha supuesto un gran avance es la [detección de la postura humana](https://github.com/CMU-Perceptual-Computing-Lab/openpose). *OpenPose: Real-time multi-person keypoint detection library for body, face, hands, and foot estimation*.\n",
    "\n",
    "![openpose](../images/demos/openpose.png)\n",
    "\n",
    "\n",
    "El proyecto [pose animator](https://github.com/yemount/pose-animator/) tiene una demo de un sistema parecido que funciona en el navegador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El módulo [mediapipe](https://mediapipe.dev/) incluye un detector de pose humana (para una sola persona) y un detector de manos y dedos que funciona bastante bien. Los probaremos en el laboratorio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminación de fondo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Máscara de detección del objeto principal en la escena: [rembg](https://github.com/danielgatis/rembg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DINO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recientemente está cobrando importancia el aprendizaje *self-supervised* [DINOV2](https://dinov2.metademolab.com/), con el que se han conseguido *universal features*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding común de texto e imagen: [Openclip](https://github.com/mlfoundations/open_clip)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimación de profundidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Midas](https://github.com/isl-org/MiDaS), [depthpro](https://github.com/apple/ml-depth-pro), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enlaces interesantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo interactiva de un perceptrón multicapa: [tensorflow playground](http://playground.tensorflow.org/)\n",
    "\n",
    "Demo interactiva de una red convolucional: [cnn-explainer](https://poloclub.github.io/cnn-explainer)\n",
    "\n",
    "Hugging Face demos: [RF-DTR](https://huggingface.co/spaces/SkalskiP/RF-DETR), [VGGT](https://huggingface.co/spaces/facebook/vggt), [InstantMesh](https://huggingface.co/spaces/TencentARC/InstantMesh)\n",
    "\n",
    "[Dino2](https://dinov2.metademolab.com/)\n",
    "\n",
    "[Transformer explainer](https://poloclub.github.io/transformer-explainer)\n",
    "\n",
    "[Papers with code](https://paperswithcode.com/)\n",
    "\n",
    "[Segment anything](https://segment-anything.com/)\n",
    "\n",
    "[Masked autoencoders](https://github.com/facebookresearch/mae)\n",
    "\n",
    "[Keras](https://keras.io/). Su autor, Chollet, tiene un libro excelente sobre deep learning, con sus correspondientes [notebooks](https://github.com/fchollet/deep-learning-with-python-notebooks).\n",
    "\n",
    "[Geron](https://github.com/ageron/handson-ml3). Autor de otro libro muy bueno, también con notebooks.\n",
    "\n",
    "[Tensorflow official models](https://github.com/tensorflow/models/tree/master/official)\n",
    "\n",
    "[Awesome deep learning](https://github.com/ChristosChristofidis/awesome-deep-learning)\n",
    "\n",
    "[9 fundamental deep learning papers](https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html)\n",
    "\n",
    "[COCO](https://cocodataset.org/#home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[house numbers dataset](http://ufldl.stanford.edu/housenumbers/)\n",
    "\n",
    "[plate recognition](https://matthewearl.github.io/2016/05/06/cnn-anpr/) (2016)\n",
    "\n",
    "[Deep reinforcement learning](https://gym.openai.com/) (Open AI Gym)\n",
    "\n",
    "[intro a CNN](https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721#.alzgfemh0) (2016)\n",
    "\n",
    "[generative models](https://openai.com/blog/generative-models/) (2016)\n",
    "\n",
    "[slides cmu](https://www.cs.cmu.edu/~epxing/Class/10715/lectures/DeepArchitectures.pdf) (2017)\n",
    "\n",
    "[curso Stanford](http://cs231n.stanford.edu/syllabus.html)\n",
    "\n",
    "[LeCun slides](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.436.894&rep=rep1&type=pdf) (2010?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con las homografías que hemos estudiado hasta ahora solo podemos añadir objetos virtuales dentro del plano de referencia.\n",
    "\n",
    "<img src=\"../images/demos/rap.png\" width=\"40%\"/>\n",
    "\n",
    "Hoy aprenderemos a añadir objetos tridimensionales, escapando del plano del marcador.\n",
    "\n",
    "<img src=\"../images/demos/ra.png\" width=\"40%\"/>\n",
    "\n",
    "Además, podemos obtener la posición de la cámara en el espacio.\n",
    "\n",
    "<img src=\"../images/demos/axes.png\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de cámara"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el tema anterior hemos visto que las transformaciones que sufren las escenas planas cuando se proyectan en una imagen son lineales (si usamos coordenadas homogéneas) y por tanto se expresan mediante una matriz H de 3x3 elementos que llamamos \"homografía\". Si conocemos esta matriz podemos dehacer la deformación de perspectiva y rectificar el plano. La matriz H se puede deducir de la transformación sufrida por 4 o más puntos. Las homografías sirven también para transferir cuadriláteros o hacer mosaicos panorámicos.\n",
    "\n",
    "Pero el mundo real es 3D. Las imágenes son el resultado de proyectar los puntos del espacio en el plano de imagen.\n",
    "\n",
    "<img src=\"../images/demos/pinhole.svg\" width=\"30%\"/>\n",
    "\n",
    "Esto significa que una escena tridimensional no se puede \"rectificar\" o \"reconstruir\" con una única imagen: cada pixel puede venir de todos los puntos 3D que están en el mismo rayo óptico. Se pierde la \"profundidad\" de los puntos. Para reconstruir la escena necesitamos varias imágenes.  La forma de hacer esto se explica en el capítulo siguiente pero antes es necesario estudiar la geometría elemental de una sola vista.\n",
    "\n",
    "---\n",
    "\n",
    "Los conceptos esenciales que aprenderemos son:\n",
    "\n",
    "- La transformación que sufre la escena 3D cuando se observa en una imagen se representa mediante una matriz 3x4 que llamamos \"**matriz de cámara**\".\n",
    "\n",
    "\n",
    "- La matriz de cámara tiene dos componentes: uno es la **matriz de calibración** $\\mathsf K$ (que obtuvimos en el ejercicio FOV) y el otro es la **pose** (posición y orientación) de la cámara en el espacio.\n",
    "\n",
    "\n",
    "- La matriz de cámara se puede deducir de las posiciones observadas en la imagen de un conjunto de puntos de referencia (p. ej. un marcador). Para ello utilizamos `cv.solvePnP`.\n",
    "\n",
    "\n",
    "\n",
    "Si recuperamos la matriz de cámara podremos añadir objetos virtuales 3D a la escena y deducir dónde está situada la cámara en el espacio.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Las tarjetas gráficas son dispositivos hardware capaces de multiplicar a una velocidad impresionante estas matrices por las coordenadas de los objetos que aparecen en el videojuego para conseguir imágenes realistas. La visión artificial trata de realizar el proceso inverso: partiendo de las imágenes deseamos obtener un modelo 3D de la escena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Herramientas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las siguientes utilidades se han usado anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2   as cv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interactive\n",
    "\n",
    "def fig(w,h):\n",
    "    plt.figure(figsize=(w,h))\n",
    "\n",
    "def readrgb(file):\n",
    "    return cv.cvtColor( cv.imread('../images/'+file), cv.COLOR_BGR2RGB) \n",
    "\n",
    "def rgb2gray(x):\n",
    "    return cv.cvtColor(x,cv.COLOR_RGB2GRAY)\n",
    "\n",
    "def imshowg(x):\n",
    "    plt.imshow(x, \"gray\")\n",
    "\n",
    "# para imprimir arrays con el número de decimales deseados\n",
    "import contextlib\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def printoptions(*args, **kwargs):\n",
    "    original = np.get_printoptions()\n",
    "    np.set_printoptions(*args, **kwargs)\n",
    "    yield \n",
    "    np.set_printoptions(**original)\n",
    "\n",
    "def sharr(a, prec=3):\n",
    "    with printoptions(precision=prec, suppress=True):\n",
    "        print(a)\n",
    "\n",
    "# muestra un polígono cuyos nodos son las filas de un array 2D\n",
    "def shcont(c, color='blue', nodes=True):\n",
    "    x = c[:,0]\n",
    "    y = c[:,1]\n",
    "    x = np.append(x,x[0])\n",
    "    y = np.append(y,y[0])\n",
    "    plt.plot(x,y,color)\n",
    "    if nodes: plt.plot(x,y,'.',color=color, markersize=11)\n",
    "\n",
    "# crea un vector (array 1D), conveniente para hacer operaciones matemáticas\n",
    "def vec(*argn):\n",
    "    return np.array(argn)\n",
    "\n",
    "# convierte un conjunto de puntos ordinarios (almacenados como filas de la matriz de entrada)\n",
    "# en coordenas homogéneas (añadimos una columna de 1)\n",
    "def homog(x):\n",
    "    ax = np.array(x)\n",
    "    uc = np.ones(ax.shape[:-1]+(1,))\n",
    "    return np.append(ax,uc,axis=-1)\n",
    "\n",
    "# convierte en coordenadas tradicionales\n",
    "def inhomog(x):\n",
    "    ax = np.array(x)\n",
    "    return ax[..., :-1] / ax[...,[-1]]\n",
    "\n",
    "# aplica una transformación homogénea h a un conjunto\n",
    "# de puntos ordinarios, almacenados como filas \n",
    "def htrans(h,x):\n",
    "    return inhomog(homog(x) @ h.T)\n",
    "\n",
    "pi = np.pi\n",
    "degree = pi/180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta clase añadimos las siguientes utilidades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para dibujar en 3D\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# crea una matriz columna a partir de elementos o de un vector 1D\n",
    "def col(*args):\n",
    "    a = args[0]\n",
    "    n = len(args)\n",
    "    if n==1 and type(a) == np.ndarray and len(a.shape) == 1:\n",
    "        return a.reshape(len(a),1)\n",
    "    return np.array(args).reshape(n,1)\n",
    "\n",
    "# crea una matriz fila\n",
    "def row(*args):\n",
    "    return col(*args).T\n",
    "\n",
    "# juntar columnas\n",
    "def jc(*args):\n",
    "    return np.hstack(args)\n",
    "\n",
    "# juntar filas\n",
    "def jr(*args):\n",
    "    return np.vstack(args)\n",
    "\n",
    "# dibujar una polilínea en 3D\n",
    "def plot3(ax,c,color):\n",
    "    x,y,z = c.T\n",
    "    ax.plot(x,y,z,color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformaciones del espacio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los **desplazamientos** y los cambios de **escala** que hemos visto en 2D tienen su análogo directo en el espacio 3D. En las **rotaciones**, además del ángulo, aparece el eje de giro. Hay varias formas de representarlas.\n",
    "\n",
    "Definimos un objeto 3D de prueba para observar el efecto de las transformaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube = np.array([\n",
    "    [0,0,0],\n",
    "    [1,0,0],\n",
    "    [1,1,0],\n",
    "    [0,1,0],\n",
    "    [0,0,0],\n",
    "\n",
    "    [0,0,1],\n",
    "    [1,0,1],\n",
    "    [1,1,1],\n",
    "    [0,1,1],\n",
    "    [0,0,1],\n",
    "\n",
    "    [1,0,1],\n",
    "    [1,0,0],\n",
    "    [1,1,0],\n",
    "    [1,1,1],\n",
    "    [0,1,1],\n",
    "    [0,1,0]\n",
    "    ])\n",
    "\n",
    "def ejes(ax,d):\n",
    "    plot3(ax,np.array([[0,0,0],[d,0,0]]),'gray')\n",
    "    plot3(ax,np.array([[0,0,0],[0,d,0]]),'gray')\n",
    "    plot3(ax,np.array([[0,0,0],[0,0,d]]),'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con matplotlib podemos visualizar el objeto desde diferentes puntos de vista:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(a=-30):\n",
    "    fig = plt.figure(figsize=(6,6)) # empezamos figura 3D\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    plot3(ax,cube,'g');\n",
    "    ax.set_xlabel('X'); ax.set_ylabel('Y'); ax.set_zlabel('Z')\n",
    "    ejes(ax,3)\n",
    "    ax.view_init(elev=30., azim=a)\n",
    "\n",
    "interactive(fun, a = (-180,180,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redefinimos las transformaciones para que funcionen en cualquier dimensión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desplazamiento\n",
    "def desp(d):\n",
    "    n = len(d)\n",
    "    D = np.eye(n+1)\n",
    "    D[:n,-1] = d\n",
    "    return D\n",
    "\n",
    "# escalado\n",
    "def scale(s):\n",
    "    return np.diag(np.append(s,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una matriz de desplazamiento en el plano tiene 3x3 elementos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desp([1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y un desplazamiento en el espacio tiene 4x4 elementos. Las coordenadas homogéneas del espacio son vectores de dimensión 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desp([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un escalado 3D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale((1,2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las rotaciones en el espacio son más delicadas. Una forma intuitiva de expresarlas es mediante un vector (eje de giro) y ángulo ([fórmula de Rodrigues](https://en.wikipedia.org/wiki/Rodrigues%27_rotation_formula)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit(v):\n",
    "    return v / np.linalg.norm(v)\n",
    "\n",
    "def rotation(v, a=None, homog=False):\n",
    "    if a==None:\n",
    "        R = cv.Rodrigues(v)[0]\n",
    "    else:\n",
    "        R = cv.Rodrigues(unit(v)*a)[0]\n",
    "    if homog:\n",
    "        Rh = np.eye(4)\n",
    "        Rh[:3,:3] = R\n",
    "        return Rh\n",
    "    else:\n",
    "        return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation((1,0,0), 45*degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se omite el argumento de ángulo se sobreentiende que va codificado como la norma del vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation((0,30*degree,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opcionalmente se puede devolver como transformación homogénea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation((1,1,1), 30*degree, homog=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos construir cualquier transformación del espacio componiendo transformaciones elementales. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = desp((5,0,3)) @ scale((2,2,2)) @ rotation((1,1,1),30*degree,homog=True)\n",
    "\n",
    "sharr(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos cómo queda el cubo cuando le aplicamos la transformarción anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube2 = htrans(H,cube)\n",
    "\n",
    "def fun(a=-30):\n",
    "    fig = plt.figure(figsize=(8,7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    plot3(ax,cube,'g')\n",
    "    plot3(ax,cube2,'red')\n",
    "    ax.set_xlabel('X'); ax.set_ylabel('Y'); ax.set_zlabel('Z')\n",
    "    ejes(ax,3)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.view_init(elev=30., azim=a)\n",
    "\n",
    "interactive(fun, a = (-180,180,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una transformación similar, el cubo preserva sus dimensiones relativas. (No hay ningún problema en aplicar transformaciones afines o proyectivas. En este último caso el cubo podría quedar como una especie de pirámide truncada.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proyección"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora cómo se puede implementar el efecto de perspectiva mediante una transformación lineal de coordenadas homogéneas.\n",
    "\n",
    "La imagen $x$ de un punto del espacio $(X,Z)$ es la intersección del plano de imagen con el rayo que une el centro óptico con dicho punto.\n",
    "\n",
    "<img src=\"../images/demos/ecbasic.svg\" width=\"40%\"/>\n",
    "\n",
    "Por semejanza de triángulos $\\frac{x}{f} = \\frac{X}{Z}$. Entonces, en este sistema de referencia, denominado \"sistema de referencia de la cámara\", en el que el eje Z apunta hacia delante, transformación de coordenadas tiene la expresión simple, aunque no lineal:\n",
    "\n",
    "$$ x = f\\; \\frac{X}{Z} \\hspace{3em} y = f\\; \\frac{Y}{Z} $$\n",
    "\n",
    "Es un cambio de escala que depende de la distancia. Los planos perpendiculares al plano de imagen no se deforman.\n",
    "\n",
    "\n",
    "Esta operación de proyección se reduce a una sencilla transformación lineal de coordenadas homogéneas con la siguiente matriz de coeficientes (válida para el caso particular $f$=1, luego veremos como reintroducir este parámetro):\n",
    "\n",
    "$$\\mathsf P = \\begin{bmatrix}1&0&0&0\\\\0&1&0&0\\\\0&0&1&0\\end{bmatrix} $$\n",
    "\n",
    "Al actuar sobre un punto del espacio produce un punto del plano cuya tercera coordenada homogénea es la profundidad $Z$. \n",
    "\n",
    "$$ \\begin{bmatrix}1&0&0&0\\\\0&1&0&0\\\\0&0&1&0\\end{bmatrix}\\; \\lambda \\begin{bmatrix}X\\\\Y\\\\Z\\\\1\\end{bmatrix} = \\lambda \\begin{bmatrix}X\\\\Y\\\\Z\\end{bmatrix}$$\n",
    "\n",
    "La división por $Z$ de la fórmula anterior no se realiza al transformar el punto sino al pasarlo después a coordenadas cartesianas. Esto solo es necesario para dibujar al final del proceso de cómputo. De hecho, las bibliotecas gráficas admiten directamente coordenadas homogéneas. Si no se necesitan las coordenadas cartesianas podemos seguir operando con el vector homogéneo multiplicándolo por otras matrices.\n",
    "\n",
    "En cierto sentido, las coordenadas homogéneas son una forma de representar un cociente $a/b$ guardando por separado el numerador y el denominador.\n",
    "\n",
    "En situaciones normales la cámara no estará justo en el origen y apuntando en dirección del eje $Z$. Como veremos dentro de un momento, la matriz de proyección P se puede combinar con un desplazamiento y una rotación para conseguir el efecto de situar la cámara en cualquier posición y orientación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz de calibración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos falta un último ingrediente para conseguir la matriz de cámara .\n",
    "\n",
    "La matriz $\\mathsf P$ proyecta los puntos 3D en el plano de imagen, que representamos en color verde en la figura siguiente.\n",
    "\n",
    "\n",
    "<img src=\"../images/demos/pinhole1.png\" width=\"30%\"/>\n",
    "\n",
    "Para su tratamiento informático tenemos que digitalizar la imagen con un sensor (un array de celdas sensibles a la luz), que produce una \"matriz de pixels\". La representamos en color rojo:\n",
    "\n",
    "\n",
    "<img src=\"../images/demos/pinhole2.png\" width=\"30%\"/>\n",
    "\n",
    "La siguiente figura muestra el plano de imagen de frente con sus dos sistemas de referencia: el de la imagen física $(x,y)$ (verde) y el de la imagen digitalizada (fila,columna) (rojo).\n",
    "\n",
    "<img src=\"../images/demos/pinhole3.png\" width=\"30%\"/>\n",
    "\n",
    "La transformación de uno a otro consiste en un cambio de escala (que tiene que ver con la resolución de la imagen) y un desplazamiento (para cambiar la posición del origen), que se expresa con una matriz $3\\times 3$ como las que hemos visto en el tema anterior. Es la **matriz de calibracion** y la representamos con la letra $\\mathsf K$. En general tiene la estructura siguiente:\n",
    "\n",
    "$$\\mathsf K =\\begin{bmatrix}f & s & c_x \\\\  0 & fr & c_y \\\\ 0 & 0 & 1\\end{bmatrix}$$\n",
    "\n",
    "Eligiendo adecuadamente los 5 parámetros $f,s,r,c_x,c_y$, la matriz K puede modelar sensores cuyas matrices de pixel están inclinadas (a), están descentradas (b), tienen pixels rectangulares (c), o cuadrados (d).\n",
    "\n",
    "<img src=\"../images/demos/matrixK.png\" width=\"70%\"/>\n",
    "\n",
    "En una cámara normal los pixels son cuadrados ($s\\simeq0$ y $r\\simeq1$) y el centro de la imagen o \"punto principal\" $(c_x,c_y)$ está aproximadamente en el centro de la matriz de pixels. Por tanto, una cámara ideal con distancia $f$ y resolución $w\\times h$ tiene una matriz de calibración muy simple\n",
    "\n",
    "$$\\mathsf K =\\begin{bmatrix}f & 0 & w/2 \\\\  0 & f & h/2 \\\\ 0 & 0 & 1\\end{bmatrix}$$\n",
    "\n",
    "donde $f$ es el único parámetro de calibración, que depende del FOV y la resolución."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este diagrama tomado de wikipedia muestra muy bien los sistemas de referencia. Las coordenadas cartesianas del punto 3D en el sistema de la cámara $\\{X_c,Y_c,Z_c\\}$ son $\\color{blue}{(X,Y,Z)}$. Su imagen tiene coordenadas físicas $\\color{turquoise}{(x,y)}$ y coordenadas de pixel $\\color{olive}{(u,v)}$ (fila $\\color{olive}{v}$ y columna $\\color{olive}{u}$). El plano de imagen está situado a la distancia $f$ del centro de proyección.\n",
    "\n",
    "<img src=\"../images/demos/pinhole_camera_model.png\" width=\"50%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa la orientación del sistema de referencia de la cámara. El eje $Z_c$ es el eje óptico, la dirección en la que apunta la cámara, y el eje $X_c$ crece hacia la derecha como de costumbre. Pero el eje $Y_c$, crece hacia abajo, igual que las coordenadas $\\color{turquoise}{y}$ y las filas $\\color{olive}{v}$. Esto es necesario para que el sistema de referencia esté \"bien orientado\" (esto significa que $Z = X\\times Y$,  siguiendo la [regla de la mano derecha](https://en.wikipedia.org/wiki/Right-hand_rule)). Si no lo hacemos así, para describir la posición de la cámara en el espacio tenemos que incluir una reflexión. Si queremos que $Z$ crezca con la distancia, el eje $Y$ debe crecer hacia abajo. Quizá sea esa la razón de que el origen de coordenadas de las matrices de pixels estén arriba a la izquierda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibración aproximada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El parámetro $f$ se puede deducir fácilmente del tamaño observado $u$ de un objeto de tamaño real $X$ situado a una distancia $Z$ también conocida. Y con la resolución horizontal (o vertical) $w$ de la imagen deducimos el campo de visión horizontal (o vertical)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/demos/pinhole2.svg\" width=\"60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente función construye la matriz de calibración de una cámara ideal a partir del FOV horizontal (en grados) y la resolución de la imagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matriz de calibración sencilla dada la\n",
    "# resolución de la imagen y el fov horizontal en grados\n",
    "def Kfov(sz,hfovd):\n",
    "    hfov = np.radians(hfovd)\n",
    "    f = 1/np.tan(hfov/2)\n",
    "    # print(f)\n",
    "    w,h = sz\n",
    "    w2 = w / 2\n",
    "    h2 = h / 2\n",
    "    return np.array([[f*w2, 0,    w2],\n",
    "                     [0,    f*w2, h2],\n",
    "                     [0,    0,    1 ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kfov((640,480),53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharr(Kfov((4000,3000),90))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente matriz de calibración es una aproximación aceptable para la cámara usada en muchas imágenes de ejemplo a lo largo del curso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = Kfov((640,480), 63 )\n",
    "sharr(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matriz $\\mathsf K$ codifica la relación entre pixels y ángulos (pero no el ángulo directamente, sino su tangente). Multiplicando las coordenadas físicas por $\\mathsf K$ obtenemos las coordenadas de pixel. Y multiplicando las coordenadas de pixel por la inversa $\\mathsf K^{-1}$ obtenemos el punto físico del plano de imagen, que podemos considerar como un representante de todo el rayo óptico 3D que se proyecta en ese pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibración precisa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En aplicaciones que requieren mucha precisión la matriz de calibracion debe estimarse con el procedimiento de optimización basado en varias imágenes de un *chessboard* (ejercicio FOV). Este método obtiene también los parámetros de distorsión radial. (En el notebook [lookup.ipynb](lookup.ipynb) se explica cómo corregir la distorsión radial en caso de necesidad, pero no nos vamos a preocupar de esto por ahora.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resultado de la calibración de mi webcam\n",
    "K = np.array([\n",
    "    [ 535.78965764,    0.,          308.50107711],\n",
    "    [   0.,          536.55857539,  220.02562845],\n",
    "    [   0.,            0.,            1.        ]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cámara sintética"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya tenemos todos los ingredientes del modelo de cámara.\n",
    "\n",
    "Una cámara es esencialmente un **centro de proyección**, un **plano de imagen** y una **malla de pixels**. El modelo matemático tiene en cuenta estos factores mediante la siguiente secuencia de transformaciones:\n",
    "\n",
    "- Mover la cámara al punto del espacio deseado con una traslación $\\mathsf T$ (que establece el centro de proyección).\n",
    "\n",
    "- Orientarla para que apunte en la dirección deseada con una rotación $\\mathsf R$ (que establece el plano de imagen).\n",
    "\n",
    "- Proyectar los puntos 3D con la matriz de proyección $\\mathsf P$\n",
    "\n",
    "- \"Digitalizar\" la imagen con la matriz de calibración $\\mathsf K$\n",
    "\n",
    "El resultado de componer esta secuencia de transformaciones homogéneas es la matriz de cámara $\\mathsf M$.\n",
    "\n",
    "$$\\mathsf M = \\mathsf {K\\, P\\, R\\, T}$$\n",
    "\n",
    "\n",
    "Es una transformación $\\mathbb P^3 \\rightarrow \\mathbb P^2$. Es decir, pasa del espacio 3D a una imagen 2D, utilizando coordenadas homogéneas.  Como entrada tiene un vector homogéneo del dimensión 4 ($\\mathbb P^3$) y como salida un vector homogéno de dimensión 3 ($\\mathbb P^2$). Por tanto la matriz de cámara $\\mathsf M$ tiene dimensión $3\\times 4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la práctica se usa más la siguiente forma compacta\n",
    "\n",
    "$$ \\mathsf M = \\mathsf K\\, [R\\, \\mid t ] $$\n",
    "\n",
    "donde $R$ es la rotación ($3\\times 3$) y $C$ es el centro de proyección $(3\\times 1)$, ambos en coordenadas cartesianas, y $t=-RC$. La notación $[\\cdot | \\cdot]$ indica una matriz por bloques. (La proyección $\\mathsf P$ no aparece explícitamente en la expresión pero sí sus efectos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La parte $[R\\mid t]$ se llama **pose** de la cámara. Codifica la posición y orientación de la cámara en el espacio. Tiene 6 grados de libertad: 3 del centro de proyección y 3 de la rotación (p. ej. 3 ángulos). Son los parámetros \"extrínsecos\" o externos.\n",
    "\n",
    "La matriz $\\mathsf K$ contiene los parámetros \"intrínsecos\" o internos, que no dependen de la posición en el espacio ni del sistema de referencia elegido para el espacio 3D. Puede tener hasta 5 grados de libertad (aunque no todos son igual de importantes).\n",
    "\n",
    "En conjunto son 11 grados de libertad, que cuadran con los 12 elementos de la matriz homogéna $\\mathsf M$ (tiene un factor de escala arbitrario)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En definitiva, las cámaras se modelan mediante matrices $\\mathsf M$ de dimensión 3x4 que transforman las coordenadas homogéneas de los puntos del espacio en las coordenadas homogéneas de los puntos de la imagen. La parte interesante de $\\mathsf M$ es la pose $[R\\mid t]$.\n",
    "\n",
    "\n",
    "Vamos a hacer unos experimentos para familiarizarnos con estos conceptos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de empezar, es conveniente disponer de una forma intuitiva de construir matrices de rotación (hacerlo directamente con ángulos es incómodo). La función siguiente construye la pose de la cámara (la parte $[R\\mid t]$, sin incluir la $\\mathsf K$) indicando su centro (`eye`), un punto al que está mirando (`target`), y la dirección vertical (`up`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construye la pose de una cámara situada en eye que apunta hacia target\n",
    "def lookat(eye, target, up = (0, 0, 1)):\n",
    "    #Las filas de la matriz de rotación \n",
    "    # son los ejes del sistema de referencia de la cámara.\n",
    "    z = np.array(target,np.float64)- eye\n",
    "    z /= np.linalg.norm(z)\n",
    "    x = np.cross(-np.array(up), z)  # \"arriba\" con el eje y creciendo hacia abajo\n",
    "    x /= np.linalg.norm(x)\n",
    "    y = np.cross(z, x)\n",
    "    R = np.float64([x, y, z])  # filas\n",
    "    #print(la.det(R))\n",
    "    tvec = -np.dot(R, eye)\n",
    "    return jc(R, col(tvec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = lookat( (2,2,2), (0,0,0) )\n",
    "sharr(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver si funciona bien (más adelante añadiremos la matriz $\\mathsf K$). Proyectaremos con ella dos objetos de referencia: el cubo y una figura plana que permite observar el sistema de referencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = (np.array(\n",
    "   [[0,   0  ],\n",
    "    [0,   1  ],\n",
    "    [0.5, 1  ],\n",
    "    [0.5, 0.5],\n",
    "    [1,   0.5],\n",
    "    [1,   0  ],\n",
    "    [0.5, 0  ]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos añadido un punto extra en $(0.5, 0)$ para señalar el eje X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig(6,6)\n",
    "shcont(ref);\n",
    "plt.axis('equal'); plt.axis(2*vec(-1,1,-1,1)); plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasamos la referencia a 3D, poniéndola \"en el suelo\", con $z=0$, para poder proyectarla con la matriz de cámara."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# añadimos una coordenada cero a todas las filas\n",
    "# para convertir un polígono del plano en un polígono\n",
    "# en el espacio, a altura z=0\n",
    "def addzerocol(x):\n",
    "    return np.hstack([x,np.zeros([len(x),1])])\n",
    "\n",
    "ref3d = addzerocol(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = htrans(M,ref3d)\n",
    "shcont(img); \n",
    "shcont(htrans(M,cube/3),'g');\n",
    "plt.axis('equal'); plt.axis(vec(-1,1,1,-1)); plt.grid()\n",
    "# OJO: invertimos el eje Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado es consistente con la cámara que hemos creado, que mira desde la posición (2,2,2) hacia el origen, con el eje $Z$ hacia arriba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estamos viendo el plano de imagen \"físico\". Una cámara real produce coordenadas de píxel, que se consiguen multiplicando por la matriz de calibración $\\mathsf K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = Kfov((640,480), 60)\n",
    "P =  lookat( (-1,-2,1), (1,1,0) )\n",
    "M = K @ P\n",
    "\n",
    "img = htrans(M,ref3d)\n",
    "\n",
    "shcont(img); \n",
    "shcont(htrans(M,cube/3),'g');\n",
    "plt.axis([0,640,480,0]); plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añadimos un slider para mover la posición de esta cámara sintética."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(a = 45):\n",
    "    cx = 2*np.cos(a*degree)\n",
    "    cy = 2*np.sin(a*degree)\n",
    "\n",
    "    K = Kfov((640,480), 65)\n",
    "    P =  lookat( (cx,cy,1), (0,0,0))\n",
    "    M = K @ P\n",
    "\n",
    "    sharr(M)\n",
    "    img = htrans(M,ref3d)\n",
    "    shcont(img); \n",
    "    shcont(htrans(M,cube/3),'g');\n",
    "    plt.axis([0,640,480,0]); plt.grid();\n",
    "\n",
    "interactive(fun, a=(-180,180,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este tipo de cámaras sintéticas se utilizan en los motores gráficos y de videojuegos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente función interactiva permite combinar el efecto de zoom y proximidad, manteniendo el tamaño del objeto (*[dolly zoom](https://en.wikipedia.org/wiki/Dolly_zoom)*). Al acercarnos se acentúa el efecto de perspectiva y puntos de fuga. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(a = 50, r = 2):\n",
    "    cx = np.cos(a*degree)\n",
    "    cy = np.sin(a*degree)\n",
    "    K = np.array([[r*320, 0,    320],\n",
    "                  [0,    r*320, 240],\n",
    "                  [0,    0,      1 ]])\n",
    "    M = K @ lookat(r*vec(cx,cy,0.5),vec(0,0,0))\n",
    "    img = htrans(M,ref3d)\n",
    "    shcont(img,color='blue')\n",
    "    shcont(htrans(M,cube/3),'green');\n",
    "    plt.axis((0,640,480,0)); plt.grid();\n",
    "\n",
    "interactive(fun, a=(0,90,5), r = (1,10,0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de la matriz de cámara"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible extraer los componentes $\\mathsf K$, $R$ y $C$ de cualquier matriz de cámara $M$, lo que permite recuperar su posición en el espacio.\n",
    "\n",
    "\n",
    "Para ello necesitamos un poco de álgebra lineal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# espacio nulo de una matriz, que sirve para obtener el centro\n",
    "# de la cámara\n",
    "def null1(M):\n",
    "    u,s,vt = np.linalg.svd(M)\n",
    "    return vt[-1,:]\n",
    "\n",
    "\n",
    "# Descomposición RQ (que no suele estar en los paquetes numéricos)\n",
    "# pero es fácil expresar en términos de la QR.\n",
    "# Descompone una matriz como producto de triangular x rotación\n",
    "# (que es justo la estructura de una matriz de cámara)\n",
    "# M = K [R |t ]\n",
    "# el trozo KR te lo da la descomposición rq \n",
    "def rq(M):\n",
    "    Q,R = np.linalg.qr(np.flipud(np.fliplr(M)).T)\n",
    "    R   = np.fliplr(np.flipud(R.T))\n",
    "    Q   = np.fliplr(np.flipud(Q.T))\n",
    "    return R,Q\n",
    "\n",
    "\n",
    "# Descomposición de la matriz de cámara como K,R,C\n",
    "def sepcam(M):\n",
    "\n",
    "    K,R = rq (M[:,:3])\n",
    "\n",
    "    # para corregir los signos de f dentro de K\n",
    "    s = np.diag(np.sign(np.diag(K)))\n",
    "\n",
    "    K = K @ s     # pongo signos positivos en K\n",
    "    K = K/K[2,2]  # y hago que el elemento 3,3 sea 1\n",
    "\n",
    "    R = s @ R     # cambio los signos igual a R para compensar\n",
    "    R = R*np.sign(np.linalg.det(R)) # y hago que tenga det = 1 (bien orientado)\n",
    "\n",
    "    # el centro de proyección es el espacio nulo de la matriz,\n",
    "    # el único punto que la cámara \"no puede ver\", porque al proyectarlo\n",
    "    # produce (0,0,0), que es un vector homogéneo \"ilegal\"\n",
    "    C = inhomog(null1(M))\n",
    "\n",
    "    return K,R,C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a comprobar que `sepcam` (\"separa cámara\") recupera correctamente los ingredientes con los que sintetizamos una cámara."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = Kfov((640,480), 60)\n",
    "P =  lookat(vec(-1,-2,1),vec(1,1,0))\n",
    "M = K @ P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K,R,C = sepcam(M)\n",
    "\n",
    "sharr(K)\n",
    "sharr(R)\n",
    "sharr(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#           [R | -RC] \n",
    "sharr(K @ jc(R , -R @ col(C)))\n",
    "sharr(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al extraer $R$ y $C$ de la matriz de cámara podemos representar en 3D su centro, el plano de imagen, etc. mediante un sencillo diagrama de alambres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esquema en 3d de una cámara\n",
    "def cameraOutline(M):\n",
    "\n",
    "    K,R,C = sepcam(M)\n",
    "\n",
    "    # formamos una transformación 3D para mover la cámara en el origen a la posición de M\n",
    "    rt = jr(jc(R, -R @ col(C)),\n",
    "            row(0,0,0,1))\n",
    "\n",
    "    sc = 0.3;\n",
    "    x = 1;\n",
    "    y = x;\n",
    "    z = 0.99;\n",
    "\n",
    "    ps =[x,    0,    z,\n",
    "         (-x), 0,    z,\n",
    "         0,    0,    z,\n",
    "         0,    1.3*y,z,\n",
    "         0,    (-y), z,\n",
    "         x,    (-y), z,\n",
    "         x,    y,    z,\n",
    "         (-x), y,    z,\n",
    "         (-x), (-y), z,\n",
    "         x,    (-y), z,\n",
    "         x,    y,    z,\n",
    "         0,    y,    z,\n",
    "         0,    0,    z,\n",
    "         0,    0,    0,\n",
    "         1,    1,    z,\n",
    "         0,    0,    0,\n",
    "         (-1), 1,    z,\n",
    "         0,    0,    0,\n",
    "         (-1), (-1), z,\n",
    "         0,    0,    0,\n",
    "         (1), (-1),  z,\n",
    "         0,    0,    0,\n",
    "         0,    0,    (2*x)]\n",
    "\n",
    "    ps = np.array(ps).reshape(-1,3)\n",
    "    return htrans(np.linalg.inv(rt), sc * ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = Kfov((640,480), 65)\n",
    "P =  lookat(vec(2,2,1),vec(0,0,0))\n",
    "M = K @ P\n",
    "\n",
    "camline = cameraOutline(M)\n",
    "\n",
    "def fun(a=-30):\n",
    "    fig = plt.figure(figsize=(12,8)) # empezamos figura 3D\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    plot3(ax,cube/3,'g');\n",
    "    plot3(ax,ref3d,'r');\n",
    "    plot3(ax,camline,'b');\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlabel('X'); ax.set_ylabel('Y'); ax.set_zlabel('Z')\n",
    "    ax.view_init(elev=30., azim=a)\n",
    "\n",
    "interactive(fun, a = (-180,180,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El eje Y de la cámara (que crece hacia abajo) sobresale un poco para que sepamos cuál es.\n",
    "\n",
    "Este esquema sirve para comprobar que las operaciones anteriores son coherentes pero las proporciones del dibujo 3D en matplotlib a veces salen incorrectas. Se puede arreglar pero es preferible usar otro tipo de herramientas gráficas (p.ej. OpenGL).  En el siguiente pantallazo sacado de un ejercicio que haremos más adelante con la webcam se ven mejor los ejes (X=azul, Y=amarillo, Z=verde) del sistema de referencia del mundo (establecido por el marcador) y el de la cámara.\n",
    "\n",
    "<img src=\"../images/demos/axes.png\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimación de la matriz de cámara a partir de correspondencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el apartado anterior hemos creado una matriz de cámara sintética y hemos comprobado que proyecta los puntos como esperamos. Es la base de los gráficos por ordenador.\n",
    "\n",
    "En visión artificial nos interesa el problema inverso: dada una imagen en la que hemos detectado un objeto conocido, queremos encontrar de forma automática una matriz $\\mathsf M$ que sea un buen modelo de la cámara que tomó la imagen. Con ella podemos añadir efectos de realidad aumentada a la escena, y también deducir la posición desde la que se tomó la imagen. Esto es útil para la navegación visual de robots y otras aplicaciones de localización.\n",
    "\n",
    "El problema de obtener la matriz de cámara a partir de correspondencias 3D $\\rightarrow$ 2D se conoce como *[camera resection](https://en.wikipedia.org/wiki/Camera_resectioning)*.\n",
    "Igual que ocurre con las homografías planas, la matriz de cámara completa se puede estimar fácilmente resolviendo un sistema de ecuaciones.\n",
    "\n",
    "Pero para mayor precisión es muy conveniente precalibrar la cámara y estimar solo la \"pose\" ($R$ y $C$).\n",
    "Para ello se utiliza la función `solvePnP` de OpenCV (\"*Perspective from $n$ Points*) que resuelve la [estimación de pose](https://en.wikipedia.org/wiki/Perspective-n-Point) de forma muy eficiente. Se necesitan solo 3 puntos correspondientes para determinar los 6 parámetros pero para que haya solución única son necesarios 4 ó más.\n",
    "\n",
    "Veamos cómo se utiliza esta función en un ejemplo sintético."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos el mismo objeto de referencia 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ref3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparamos una cámara sintética:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = Kfov((640,480), 67)                        # calibración\n",
    "P = lookat(vec(-1,-2,1),vec(1,1,0))            # pose\n",
    "M = K @ P                                      # matriz de cámara"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos la imagen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view = htrans(M,ref3d)\n",
    "view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para recuperar la pose necesitamos la matriz de calibración $\\mathsf K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok,rvec,tvec = cv.solvePnP(ref3d, view, K, (0,0,0,0))\n",
    "# necesita K y los parámetros de distorsión radial (suponemos que no tiene)\n",
    "\n",
    "print(ok)  # ha tenido éxito o no en la optimización.\n",
    "print(rvec) # es una representación compacta de la rotación.\n",
    "print(tvec) # == -RC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado viene \"codificado\", pero es fácil convertirlo a la forma de matriz que hemos usado anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R,_ = cv.Rodrigues(rvec)\n",
    "\n",
    "sharr(R)                      # orientación\n",
    "sharr(-R.T @ tvec)            # centro de proyección\n",
    "sharr(K @ jc(R,tvec))         # matriz de cámara estimada por solvePnP\n",
    "sharr(M)                      # matriz real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es conveniente empaquetar el procedimiento anterior en una función `pose`, que además de la matriz de cámara estimada devuelve el error cometido por ella con esas correspondencias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mide el error de una transformación (p.ej. una cámara)\n",
    "# rms = root mean squared error\n",
    "# \"reprojection error\"\n",
    "def rmsreproj(view, model, transf):\n",
    "    err = view - htrans(transf,model)\n",
    "    return np.sqrt(np.mean(err.flatten()**2))\n",
    "\n",
    "def pose(K, image, model):\n",
    "    ok,rvec,tvec = cv.solvePnP(model, image, K, (0,0,0,0))\n",
    "    if not ok:\n",
    "        return 1e6, None\n",
    "    R,_ = cv.Rodrigues(rvec)\n",
    "    M = K @ jc(R,tvec)\n",
    "    rms = rmsreproj(image,model,M)\n",
    "    return rms, M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    M estimada\n",
    "rms, Me = pose(K,view,ref3d)\n",
    "\n",
    "sharr(Me)\n",
    "print(rms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtiene un error de reproyección que es un cero numérico, ya que los puntos de imagen se han generado con el modelo sin ningún ruido de medida. En la práctica los puntos de interés en las imágenes se detectan con un cierto error.\n",
    "\n",
    "Vamos a añadir ruido artificial a los puntos de imagen para comprobar cómo se comporta el algoritmo PnP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(shape):\n",
    "    import numpy.random as rnd\n",
    "    return rnd.standard_normal(shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añadimos ruido gausiano de $\\sigma =3$ pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_view = view + 3*noise(view.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shcont(view, color='lightgray', nodes=False)\n",
    "shcont(noisy_view)\n",
    "plt.axis((0,640,480,0)); plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms, Me = pose(K, noisy_view, ref3d)\n",
    "\n",
    "sharr(Me)\n",
    "print(rms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cámara obtenida es bastante aceptable teniendo en cuenta el nivel de ruido. P. ej., el centro de proyección estimado es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sepcam(Me)[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mientras que el centro real es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sepcam(M)[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La calidad de la estimación aumenta con el número de correspondencias, siempre que no existan *outliers*, en cuyo caso es imprescindible emplear un método robusto como RANSAC (ya comentado en el tema anterior), que también está disponible en la función `cv.solvePnP`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realidad aumentada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante la técnica anterior es posible estimar la cámara que ha capturado una imagen real siempre que consigamos identificar $n\\geq4$ puntos de referencia conocidos. (Con $n=3$ se puede resolver pero hay varias soluciones). Con ella podemos proyectar en la imagen cualquier objeto 3D con la perspectiva correcta.\n",
    "\n",
    "Lo ideal sería utilizar una referencia 3D grande, con puntos en un amplio rango de distancias. Pero por simplicidad en el ejemplo siguiente trabajaremos con marcadores poligonales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos unas cuantas funciones auxiliares para detectar contornos prometedores. Se trata esencialmente del mismo método que usamos en la práctica anterior. Está disponible en umucv pero lo incluimos aquí como recordatorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilidades de extracción de contornos (disponibles en umucv)\n",
    "\n",
    "# area, con signo positivo si el contorno se recorre \"counterclockwise\"\n",
    "def orientation(x):\n",
    "    return cv.contourArea(x.astype(np.float32),oriented=True)\n",
    "\n",
    "# ratio area/perímetro^2, normalizado para que 100 (el arg es %) = círculo\n",
    "def redondez(c):\n",
    "    p = cv.arcLength(c.astype(np.float32),closed=True)\n",
    "    oa = orientation(c)\n",
    "    if p>0:\n",
    "        return oa, 100*4*np.pi*abs(oa)/p**2\n",
    "    else:\n",
    "        return 0,0\n",
    "\n",
    "def boundingBox(c):\n",
    "    (x1, y1), (x2, y2) = c.min(0), c.max(0)\n",
    "    return (x1, y1), (x2, y2)\n",
    "\n",
    "# comprobar que el contorno no se sale de la imagen\n",
    "def internal(c,h,w):\n",
    "    (x1, y1), (x2, y2) = boundingBox(c)\n",
    "    return x1>1 and x2 < w-2 and y1 > 1 and y2 < h-2\n",
    "\n",
    "# reducción de nodos\n",
    "def redu(c,eps=0.5):\n",
    "    red = cv.approxPolyDP(c,eps,True)\n",
    "    return red.reshape(-1,2)\n",
    "\n",
    "# intenta detectar polígonos de n lados\n",
    "def polygons(cs,n,prec=2):\n",
    "    rs = [ redu(c,prec) for c in cs ]\n",
    "    return [ r for r in rs if r.shape[0] == n ]\n",
    "\n",
    "# detecta siluetas oscuras que no sean muy pequeñas ni demasiado alargadas\n",
    "def extractContours(g, minarea=10, minredon=25, reduprec=1):\n",
    "    #gt = cv.adaptiveThreshold(g,255,cv.ADAPTIVE_THRESH_GAUSSIAN_C, cv.THRESH_BINARY,101,-10)\n",
    "    ret, gt = cv.threshold(g,189,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "\n",
    "    contours = cv.findContours(gt, cv.RETR_TREE,cv.CHAIN_APPROX_SIMPLE)[-2]\n",
    "\n",
    "    h,w = g.shape\n",
    "\n",
    "    tharea = (min(h,w)*minarea/100.)**2 \n",
    "\n",
    "    def good(c):\n",
    "        oa,r = redondez(c)\n",
    "        black = oa > 0 # and positive orientation\n",
    "        return black and abs(oa) >= tharea and r > minredon\n",
    "\n",
    "    ok = [redu(c.reshape(-1,2),reduprec) for c in contours if good(c)]\n",
    "    return [ c for c in ok if internal(c,h,w) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente imagen de prueba contiene varios marcadores con la forma que hemos estado utilizando. Intentamos detectarlos como polígonos de 6 lados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = readrgb('marker.png')\n",
    "g = rgb2gray(img)\n",
    "\n",
    "conts = extractContours(g, reduprec=3)\n",
    "good = polygons(conts,6)\n",
    "\n",
    "fig(12,8)\n",
    "plt.imshow(img); ax = plt.axis();\n",
    "for c in conts:\n",
    "    shcont(c,nodes=False,color='blue')\n",
    "for g in good:\n",
    "    shcont(g,color='green')\n",
    "plt.axis(ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El que tiene los bordes en ziz-zag no se reduce a 6 lados con el nivel de reducción especificado. Observa lo que ocurre con reduprec=10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método tiene poca precisión en la localización y será muy inestable cuando trabajemos con cámaras en vivo. En cualquier caso, sigamos adelante a ver lo que conseguimos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los vértices del marcador que se ve abajo a la izquierda son los siguientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objeto 3D de referencia es el objeto `ref3d` anterior pero quitándole el último punto auxiliar que añadimos para marcar el eje X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker = ref3d[:-1]\n",
    "\n",
    "marker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matriz de calibración de la cámara es aproximadamente esta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = Kfov((640,480), 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos la pose de la cámara:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose(K, good[0].astype(float), marker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer número del resultado es el error del modelo: 26 pixels. Es enorme. Esto se debe a que el polígono detectado \"no empieza\" en el mismo vértice que el modelo. Las correspondencias son incorrectas. Esto ya nos ocurrió en la práctica anterior. De nuevo, probamos todos los puntos de partida y nos quedamos con el mejor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rots(c):\n",
    "    return [np.roll(c,k,0) for k in range(len(c))]\n",
    "\n",
    "rots(good[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las rotaciones consigue un buen ajuste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ pose(K, v.astype(float), marker) for v in rots(good[0]) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo encapsulamos en una función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probamos todas las asociaciones de puntos imagen con modelo\n",
    "# y nos quedamos con la que produzca menos error\n",
    "def bestPose(K,view,model):\n",
    "    poses = [ pose(K, v.astype(float), model) for v in rots(view) ]\n",
    "    return sorted(poses,key=lambda p: p[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err,Me = bestPose(K,good[0],marker)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dibujamos la cámara estimada en el espacio junto con el marcador detectado y comprobamos visualmente que la posición relativa es correcta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camline = cameraOutline(Me)\n",
    "\n",
    "def fun(a=-30):\n",
    "    fig = plt.figure(figsize=(8,7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    plot3(ax,marker,'r');\n",
    "    plot3(ax,camline,'b');\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlabel('X'); ax.set_ylabel('Y'); ax.set_zlabel('Z')\n",
    "    ax.view_init(elev=30., azim=a)\n",
    "\n",
    "interactive(fun, a = (-180,180,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación proyectamos el marcador sobre la imagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig(12,8)\n",
    "plt.imshow(img); ax = plt.axis();\n",
    "\n",
    "shcont( htrans(Me,marker), color='red')\n",
    "\n",
    "plt.axis(ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aparece en la posición correcta. No hemos dibujado el polígono detectado, sino la proyección del marcador real con la cámara estimada. El hecho de que quede tan bien ajustado indica que es una aproximación aceptable a la cámara real. Podemos confirmarlo con puntos conocidos que no hemos usado en la estimación, como por ejemplo el cuadro gris que hay dibujado en el marcador más grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err,Me = bestPose(K,good[2],marker)\n",
    "print(err)\n",
    "\n",
    "fig(12,8)\n",
    "plt.imshow(img); ax = plt.axis();\n",
    "\n",
    "shcont( htrans(Me, [(1,1,0)] ), color='red')\n",
    "\n",
    "plt.axis(ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo importante es que la matriz de cámara estimada, aunque se ha obtenido con información de un solo plano de la escena, sirve para proyectar puntos de todo el espacio y permite añadir objetos artificiales a cualquier altura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig(12,8)\n",
    "plt.imshow(img); ax = plt.axis();\n",
    "shcont( htrans(Me, cube * (1/4,1/4,0.75) + (0.75,0.75,0)), nodes=False,color='blue')\n",
    "shcont( htrans(Me, cube/2), nodes=False,color='red')\n",
    "plt.axis(ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada referencia detectada da lugar a su propia matriz de cámara estimada. Todas ellas se refieren a la misma cámara física, pero cada matriz está expresada en un sistema de referencia distinto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig(12,8)\n",
    "plt.imshow(img); ax = plt.axis();\n",
    "for g in good:\n",
    "    err,Me = bestPose(K,g,marker)\n",
    "    if err < 2:\n",
    "        shcont(htrans(Me,cube/2),nodes=False,color='red')\n",
    "plt.axis(ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ojo: como en la imagen el eje Y está cambiado, para que la pose se calcule con los ejes correctos el sentido de recorrido del marcador debe ser el contrario al de la imagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orientation(marker[:,:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orientation(good[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos otro ejemplo. Vamos a intentar detectar las flechas pintadas en el carril bici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.resize(readrgb('arrow2.jpg'),(640,480))\n",
    "g = 255-rgb2gray(img)\n",
    "\n",
    "conts = extractContours(g, minredon=10, reduprec=5)\n",
    "good = polygons(conts,7)\n",
    "\n",
    "fig(12,8)\n",
    "plt.imshow(img); ax = plt.axis();\n",
    "for c in conts:\n",
    "    shcont(c,nodes=False,color='blue')\n",
    "for g in good:\n",
    "    shcont(g,color='red')\n",
    "plt.axis(ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomamos una imagen frontal para sacar de ella el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgm = cv.resize(readrgb('arrow1.jpg'),(320,240))\n",
    "g = 255-rgb2gray(imgm)\n",
    "\n",
    "conts = extractContours(g, minredon=10)\n",
    "\n",
    "fig(6,6)\n",
    "plt.imshow(imgm); ax = plt.axis();\n",
    "for c in conts:\n",
    "    shcont(c,nodes=False,color='orange')\n",
    "plt.axis(ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por prueba y error cambiamos el nivel de reducción hasta que se detecte el polígono."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow = polygons([redu(c,3) for c in conts],7)[0]\n",
    "\n",
    "shcont(arrow)\n",
    "\n",
    "plt.axis(ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajustamos su tamaño para que mida una unidad de ancho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoscale(cont):\n",
    "    (x1, y1), (x2, y2) = cont.min(0), cont.max(0)\n",
    "    s = max(x2-x1,y2-y1)\n",
    "    c = vec(x1+x2,y1+y2)/2\n",
    "    h = scale(1/vec(s,s)) @ desp(-c)\n",
    "    return htrans(h,cont)\n",
    "\n",
    "arrow = autoscale(arrow)\n",
    "\n",
    "shcont(arrow); plt.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubito = (cube-row(0.5,0.5,0))/3\n",
    "\n",
    "fig(12,8)\n",
    "plt.imshow(img); ax = plt.axis();\n",
    "for g in [good[0]]:\n",
    "    err,Me = bestPose(K,g,np.flipud(addzerocol(arrow)))\n",
    "    if err < 2:\n",
    "        shcont(htrans(Me,cubito),nodes=False,color='red')\n",
    "plt.axis(ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio: añade una flecha virtual en un poste vertical que apunte en la misma dirección que la flecha real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realidad aumentada con la webcam en vivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las posibilidades son infinitas.\n",
    "\n",
    "<video controls src=\"../images/demos/dynamic.mp4\"></video>\n",
    "\n",
    "En la sesión práctica haremos paso a paso una pequeña aplicación para mostrar en 3D y en tiempo real la posición de la cámara y efectos de realidad aumentada a partir de un marcador artificial. Es la base de el ejercico **RA**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando OpenGL se pueden conseguir efectos 3D más realistas. \n",
    "\n",
    "<img src=\"../images/demos/opengl-ra1.png\" width=\"40%\"/>\n",
    "\n",
    "<img src=\"../images/demos/opengl-ra2.png\" width=\"40%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la actualidad hay numerosos [\"frameworks\" de realidad aumentada](https://en.wikipedia.org/wiki/List_of_augmented_reality_software). Algunos admiten escenas naturales, sin necesidad de marcadores."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

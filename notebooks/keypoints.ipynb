{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Keypoints*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta lección experimentaremos con la propiedad visual más importante: los *keypoints* o \"[puntos de interés](https://en.wikipedia.org/wiki/Interest_point_detection)\" y su aplicación al reconocimiento de objetos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el capítulo anterior dedicado al flujo óptico hemos visto que las esquinas o *corners* son fácilmente localizables con precisión en imágenes sucesivas. Recuerda el ejemplo [lk_track.py](../code/LK/lk_track.py), en el que se muestran las [trayectorias](../images/demos/tracks.png).\n",
    "Esta técnica es muy útil para determinadas aplicaciones, pero cuando se cortan los enlaces (por un movimiento brusco de la cámara o porque el punto se sale de la escena) ya no hay forma de continuar la secuencia.\n",
    "\n",
    "Ahora vamos a dar un paso más: buscaremos zonas características que puedan identificarse en imágenes completamente nuevas aunque se produzcan desplazamientos, giros o cambios de tamaño. En el vídeo siguiente tenemos a la derecha un objeto que queremos reconocer, y a la izquierda la imagen en tiempo real de la webcam. En gris se muestran los \"keypoints\" detectados y en verde las \"correspondencias\" (*matching*) encontradas. Si hay \"muchas\" correspondencias (el número se muestra en el segundo recuadro) podemos suponer que el objeto está en la escena, aunque alguna correspondencia sea errónea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video controls src=\"https://github.com/albertoruiz/umucv/raw/refs/heads/master/images/demos/sift/matching2.mp4\" style=\"border:0\"></video> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para conseguir esto es necesario:\n",
    "\n",
    "- Detectar zonas \"informativas\", donde la superficie de imagen sea abrupta, no lineal. No queremos zonas uniformes ni bordes suaves.\n",
    "\n",
    "- Estas zonas deben tener un tamaño característico, o \"escala\", que pueda calcularse fácilmente, para conseguir invarianza al tamaño.\n",
    "\n",
    "- Hay que calcular un vector de características o \"descriptor\" de cada punto para medir la similitud entre ellos y encontrar correspondencias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas y funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy             as np\n",
    "import cv2               as cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def fig(w,h):\n",
    "    plt.figure(figsize=(w,h))\n",
    "\n",
    "def readrgb(file):\n",
    "    return cv.cvtColor( cv.imread(\"../images/\"+file), cv.COLOR_BGR2RGB) \n",
    "\n",
    "def rgb2gray(x):\n",
    "    return cv.cvtColor(x,cv.COLOR_RGB2GRAY)\n",
    "\n",
    "def gray2float(x):\n",
    "    return x.astype(float) / 255\n",
    "\n",
    "# para ver imágenes monocromas autoescalando el rango\n",
    "def imshowg(x):\n",
    "    plt.imshow(x, 'gray')\n",
    "\n",
    "# para ver imágenes monocromas de float con rango fijo\n",
    "def imshowf(x):\n",
    "    plt.imshow(x, 'gray', vmin = 0, vmax=1)\n",
    "\n",
    "# para ver imágenes con signo\n",
    "def imshows(x,r=1):\n",
    "    plt.imshow(x, 'gray', vmin = -r, vmax=r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Hessian*: detector de no linealidad\n",
    "\n",
    "Las zonas de la imagen cuya superficie de color o nivel de gris es localmente plana no aportan mucha información. Corresponden a regiones uniformes o que cambian suavemente. Una forma de detectar zonas interesantes es buscar segundas derivadas intensas.\n",
    "\n",
    "Con el gradiente (primeras derivadas) y el Hessiano (segundas derivadas) construimos un modelo cuadrático de la imagen en un pequeño entorno de cada pixel. Si fuera unidimensional y centrado en el origen sería algo como $I(x)\\simeq a+bx+\\frac{1}{2}cx^2$. En dos dimensiones:\n",
    "\n",
    "$$p = \\begin{bmatrix}x\\\\y\\end{bmatrix} \\;\\;\\;\\; \\Delta p = p - p_0$$\n",
    "\n",
    "$$I(p) \\simeq I(p_0) + \\nabla I\\, \\Delta p + \\frac{1}{2} \\Delta p ^ T \\, H \\, \\Delta p $$\n",
    "\n",
    "$$\\nabla I = \\begin{bmatrix}\\frac{\\partial I}{\\partial x}&\\frac{\\partial I}{\\partial y}\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "$$H=\\begin{bmatrix}\\frac{\\partial^2 I}{\\partial x^2}&\\frac{\\partial^2 I}{\\partial x\\partial y}\\\\\n",
    "                   \\frac{\\partial^2 I}{\\partial y\\partial x}&\\frac{\\partial^2 I}{\\partial y^2}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "A continuación se muestran distintos tipos de modelos locales (para versiones muy suavizadas de la imagen original):\n",
    "\n",
    "<table border=1>\n",
    "  <tr>\n",
    "    <td><img src=\"https://github.com/albertoruiz/umucv/raw/refs/heads/master/images/demos/quadmod/H3.png\" width=\"160px\"/></td>\n",
    "    <td><img src=\"https://github.com/albertoruiz/umucv/raw/refs/heads/master/images/demos/quadmod/H4.png\" width=\"160px\"/></td>\n",
    "    <td><img src=\"https://github.com/albertoruiz/umucv/raw/refs/heads/master/images/demos/quadmod/H5.png\" width=\"160px\"/></td>\n",
    "  </tr><tr>\n",
    "    <td><img src=\"https://github.com/albertoruiz/umucv/raw/refs/heads/master/images/demos/quadmod/H1.png\" width=\"160px\"/></td>\n",
    "    <td><img src=\"https://github.com/albertoruiz/umucv/raw/refs/heads/master/images/demos/quadmod/H2.png\" width=\"160px\"/></td>\n",
    "    <td><img src=\"https://github.com/albertoruiz/umucv/raw/refs/heads/master/images/demos/quadmod/H6.png\" width=\"160px\"/></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "En el determinante Hessiano $det(H)$ indica en qué medida es necesario el término cuadrático (no lineal) para modelar localmente la superficie de imagen.\n",
    "\n",
    "También se podría usar el Laplaciano, pero es menos útil porque su respuesta no permite distinguir bordes intensos de estructuras más localizadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Los elementos de $\\nabla I$ y $H$ se calculan mediante filtros de derivación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(x):\n",
    "    gx = cv.Sobel(x,-1,1,0)/8\n",
    "    gy = cv.Sobel(x,-1,0,1)/8\n",
    "    return gx,gy\n",
    "\n",
    "def grad2(x):\n",
    "    gx,gy = grad(x)\n",
    "    gxx,gxy = grad(gx)\n",
    "    #gxx = cv.Sobel(x,-1,2,0); gxy = cv.Sobel(x,-1,1,1)\n",
    "    gyy = cv.Sobel(gy,-1,0,1)/8\n",
    "    #gyy = cv.Sobel(x,-1,0,2)\n",
    "    return gx,gy,gxx,gyy,gxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La imagen siempre se analiza a una cierta escala. Para ello se hará un suavizado gaussiano previo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(s,x):\n",
    "    return cv.GaussianBlur(x,(0,0), s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos una imagen de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img = readrgb('d2.jpg')[0:500,0:500]\n",
    "img = readrgb('pano/pano001.jpg')\n",
    "\n",
    "x = gray2float(rgb2gray(img))\n",
    "imshowg(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gx,gy,gxx,gyy,gxy = grad2(gaussian(3,x))\n",
    "h = gxx*gyy-gxy**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig(12,8); plt.suptitle('segundas derivadas')\n",
    "plt.subplot(2,2,1); imshowg(gxx); plt.title('$\\\\partial ^2 I/ \\\\partial x^2$')\n",
    "plt.subplot(2,2,2); imshowg(gxy); plt.title('$\\\\partial ^2 I/ \\\\partial x \\\\partial y$')\n",
    "plt.subplot(2,2,4); imshowg(gyy); plt.title('$\\\\partial ^2 I/ \\\\partial y^2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El signo del determinante indica si la zona es una mancha compacta, más clara u oscura que su entorno, (positivo) o un punto de silla (negativo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig(18,6)\n",
    "plt.subplot(1,2,1)\n",
    "imshows(np.maximum(0,h),0.0001); plt.title('blobs')\n",
    "plt.subplot(1,2,2)\n",
    "imshows(np.minimum(0,h),0.0001); plt.title('saddlepoints');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una forma rápida de encontrar los máximos del detector (*non maximum supression*) puede ser la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(x, t = 0.1):\n",
    "    m = cv.dilate(x, np.ones((5,5),np.uint8))  # filtro de máximo\n",
    "    h = np.max(m)\n",
    "    return (x == m) & (x > t*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = nms(-h)\n",
    "fig(12,8)\n",
    "#imshowg(k)\n",
    "plt.imshow(k, 'gray', interpolation='bicubic');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo anterior es una imagen booleana. De ella extraemos las coordenadas de los puntos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py,px = np.where(k)\n",
    "\n",
    "fig(12,8)\n",
    "plt.imshow(img); #ax = plt.axis();\n",
    "plt.plot(px,py,'.r'); #plt.axis(ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disgresión: los puntos de hessiano grande tienen la información relevante, son zonas no lineales y con ellas se puede reconstruir una versión aproximada de la imagen, de forma parecida a lo que hicimos con los bordes de Canny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = nms(abs(h),0.05)\n",
    "\n",
    "mask = cv.dilate(k.astype(np.uint8), np.ones((5,5),np.uint8))\n",
    "xx = img.copy()\n",
    "xx[mask==0] = 0,0,0\n",
    "dst = cv.inpaint(img,1-mask,3,cv.INPAINT_NS);\n",
    "\n",
    "fig(15,6)\n",
    "plt.subplot(1,2,1); plt.imshow(xx);\n",
    "plt.subplot(1,2,2); plt.imshow(dst);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detección automática de escala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los puntos esquina (*corners*) que vimos en el capítulo anterior tienen un comportamiento parecido, pero como veremos a continuación, el enfoque basado en el Hessiano permite asignar un tamaño característico a los puntos de interés. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La intensidad del detector depende de la escala de análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disk = gray2float(rgb2gray(readrgb('disk1.jpg')))\n",
    "\n",
    "imshowg(disk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### det Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hess(x):\n",
    "    _,_,gxx,gyy,gxy = grad2(x)\n",
    "    h = gxx*gyy-gxy**2\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interactive\n",
    "\n",
    "mxs = 50\n",
    "scales = [1.1**k for k in range(-1,mxs+1)]\n",
    "hs = np.array([hess(gaussian(s,disk)) for s in scales])\n",
    "\n",
    "def fun(k=17):\n",
    "    imshows(hs[k],0.00001)\n",
    "    plt.title('$\\sigma={:.1f}$'.format(scales[k]))\n",
    "\n",
    "interactive(fun, k=(0,mxs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto sugiere la posibilidad de detectar automáticamente la escala óptima de cada punto (el tamaño de su \"área de influencia\"). La idea es construir el espacio de escala $(x,y,\\sigma)$ de la imagen (una [secuencia de suavizados][1] cada vez mayores) y encontrar los puntos de ese espacio 3D discretizado que son máximos locales (los que tienen una respuesta mayor que sus 9 vecinos).\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Pyramid_(image_processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante normalizar el valor de Hessiano para que tenga respuestas comparables a distintas escalas. Se puede demostrar que la normalización adecuada para el Hessiano es $\\sigma^4$ y que se obtendrán máximos locales en objetos redondeados de radio $\\sigma \\sqrt{2}$. Lo comprobamos con un par de círculos de tamaño conocido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circle(sz=500, rad=50):\n",
    "    r = np.arange(-sz//2,sz//2)\n",
    "    x = r.reshape(-1,1)\n",
    "    y = r.reshape(1,-1)\n",
    "    img = x**2 + y**2 <= rad**2\n",
    "    return img.astype(float)\n",
    "\n",
    "circle1 = circle(500,80)\n",
    "circle2 = circle(250,30)\n",
    "\n",
    "fig(12,4)\n",
    "plt.subplot(1,2,1); plt.imshow(circle1,'gray'); plt.title('radio 80 px')\n",
    "plt.subplot(1,2,2); plt.imshow(circle2,'gray'); plt.title('radio 30 px');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs1 = np.array([s**4*hess(gaussian(s,circle1))[250,250] for s in scales])\n",
    "s1 = scales[np.argmax(hs1)]\n",
    "hs2 = np.array([s**4*hess(gaussian(s,circle2))[125,125] for s in scales])\n",
    "s2 = scales[np.argmax(hs2)]\n",
    "print('rad: ',np.sqrt(2)*np.array([s1,s2]))\n",
    "rads = np.sqrt(2)*np.array(scales)\n",
    "fig(12,4)\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(hs1)\n",
    "plt.plot(hs2); plt.xlabel('etapa');\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(rads,hs1,'.-')\n",
    "plt.plot(rads,hs2,'.-'); plt.xlabel('$\\sqrt{2}\\sigma$');\n",
    "plt.suptitle('scale-normalized det Hessian');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los tamaños se detectan muy bien, teniendo en cuenta que la escala está discretizada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante este procedimiento vamos detectar los círculos de la imagen siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circles = cv.resize(readrgb('circles.jpg'),(0,0), fx=0.1, fy=0.1)\n",
    "circles = 1-np.pad(gray2float(rgb2gray(circles)),100,mode='constant',constant_values=1)\n",
    "\n",
    "x = circles\n",
    "imshowg(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = np.array([s**4*hess(gaussian(s,x)) for s in scales])\n",
    "\n",
    "def fun(k):\n",
    "    imshows(hs[k],r=0.05)\n",
    "    plt.title('$\\sigma={:.1f}$'.format(scales[k]))\n",
    "\n",
    "interactive(fun, k=(0,mxs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos los extremos más intensos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.ndimage\n",
    "\n",
    "ks = scipy.ndimage.maximum_filter(hs,3)\n",
    "\n",
    "S,R,C = np.where((ks==hs) & (hs >0.05))\n",
    "#print(S,R,C)\n",
    "\n",
    "fig(7,7)\n",
    "imshowg(x)\n",
    "ax = plt.gca()\n",
    "for s,r,c in zip(S,R,C):\n",
    "        ax.add_patch(plt.Circle((c,r), np.sqrt(2)*scales[s],color='red',fill=False,lw=3));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La detección es excelente. En imágenes naturales no hay círculos perfectos, pero se detectarán \"blobs\" (manchas más o menos redondeadas) de diferentes tamaños."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplacian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos la respuesta del Laplaciano, que es un detector más simple de no linealidad en la imagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lap(x):\n",
    "    _,_,gxx,gyy,_ = grad2(x)\n",
    "    l = gxx + gyy\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La normalización de Laplaciano es $\\sigma^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ls = np.array([s**2*lap(gaussian(s,x)) for s in scales])\n",
    "\n",
    "def fun(k):\n",
    "    imshows(Ls[k],r=0.5)\n",
    "    plt.title('$\\sigma={:.1f}$'.format(scales[k]))\n",
    "\n",
    "interactive(fun, k=(0,mxs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De nuevo, mostramos los extremos más intensos. En este caso nos fijamos en los mínimos, dado que los \"blobs\" son negros sobre fondo blanco. Si fuera al revés, seleccionaríamos máximos locales. (El determinante Hessiano positivo detecta ambos tipos de blobs, mientras que el negativo detecta los puntos de silla)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = scipy.ndimage.minimum_filter(Ls,3)\n",
    "\n",
    "S,R,C = np.where((ks==Ls) & (Ls <-0.3))\n",
    "\n",
    "fig(7,7)\n",
    "imshowg(x)\n",
    "ax = plt.gca()\n",
    "for s,r,c in zip(S,R,C):\n",
    "        ax.add_patch(plt.Circle((c,r), np.sqrt(2)*scales[s],color='red',fill=False,lw=3));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La detección es igual de precisa y el radio tiene la misma relación con la escala de máxima respuesta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestran animaciones del Laplaciano para escalas crecientes. Cuando se alcanza el máximo local se marca un círculo con la escala detectada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video controls src=\"https://github.com/albertoruiz/umucv/raw/refs/heads/master/images/demos/lapcir2.mp4\" style=\"border:0\"></video> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video controls src=\"https://github.com/albertoruiz/umucv/raw/refs/heads/master/images/demos/lapjp2.mp4\" style=\"border:0\"></video> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DoG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, vamos a calcular una aproximación al Laplaciano computacionalmente eficiente, basada en la diferencia de niveles consecutivos de suavizado. Es el operador [DoG](https://en.wikipedia.org/wiki/Blob_detection#The_difference_of_Gaussians_approach)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 6\n",
    "deltasigma = 1\n",
    "\n",
    "s1 = gaussian(sigma,x)\n",
    "s2 = gaussian(sigma+deltasigma, x)\n",
    "\n",
    "al = (s2-s1)*sigma/deltasigma\n",
    "l = lap(gaussian(sigma,x))*sigma**2\n",
    "\n",
    "fig(10,5)\n",
    "plt.subplot(1,2,1); plt.imshow(l,'gray'); plt.title('Laplaciano')\n",
    "plt.subplot(1,2,2); plt.imshow(al,'gray'); plt.title('DoG');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig(10,4)\n",
    "plt.plot(al[275],label='$\\\\frac{\\sigma}{\\Delta \\sigma}(G_{\\sigma + \\Delta \\sigma}(I) - G_{\\sigma}(I)$')\n",
    "plt.plot(l[275], label='$\\sigma^2 \\\\nabla^2 G_\\sigma(I)$');\n",
    "plt.legend(fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el espacio de escala. Para que el proceso sea más eficiente habría que ir reduciendo la imagen de tamaño, pero nosotros ahora no lo hacemos para poder comparar las etapas de forma directa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mxs = 20\n",
    "scales = [1.4**k for k in range(-1,mxs+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyr = np.array([gaussian(s,x) for s in scales])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(k=7):\n",
    "    plt.imshow(pyr[k],'gray',vmin=pyr.min(),vmax=pyr.max());\n",
    "    plt.title('$\\sigma={:.1f}$'.format(scales[k]))\n",
    "\n",
    "interactive(fun, k=(0,mxs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora calculamos las diferencias entre ellas. Gracias a que las escalas van aumentando de forma exponencial el factor de ajuste $\\sigma/\\Delta\\sigma$ es constante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 1/(1.4-1)\n",
    "\n",
    "dog = np.array([(xa-x)*f for x,xa in zip(pyr,pyr[1:])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(k=7):\n",
    "    plt.imshow(dog[k],'gray',vmin=dog.min(),vmax=dog.max());\n",
    "    plt.title('$\\sigma={:.1f}$'.format(scales[k]))\n",
    "\n",
    "interactive(fun, k=(0,mxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = scipy.ndimage.minimum_filter(dog,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S,R,C = np.where((ks==dog) & (abs(dog) >0.3))\n",
    "S,R,C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig(7,7)\n",
    "imshowg(x)\n",
    "ax = plt.gca()\n",
    "for s,r,c in zip(S,R,C):\n",
    "        ax.add_patch(plt.Circle((c,r), np.sqrt(2)*scales[s],color='red',fill=False,lw=2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A medida que $\\Delta \\sigma$ aumenta la detección de tamaño pierde algo de precisión pero el resultado es muy aceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ejemplo](https://www.nature.com/articles/s41598-018-19379-x)\n",
    "\n",
    "- [automatic scale selection](https://ags.cs.uni-kl.de/fileadmin/inf_ags/opt-ss14/OPT_SS2014_lec03.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIFT: *scale invariant feature transform*\n",
    "\n",
    "Es uno de los [métodos más conocidos](https://en.wikipedia.org/wiki/Scale-invariant_feature_transform) para extraer puntos de interés. Obtiene puntos característicos con una escala y orientación, y con un *descriptor* de apariencia que permite calcular coincidencias entre puntos de diferentes imágenes. Esto permite resolver muchos problemas de visión artificial, tanto de reconocimiento de objetos como de geometría visual. La detección se basa en máximos locales del determinante Hessiano en el espacio de escala aprovechando el método DoG anterior, y el descriptor es un histograma de gradientes orientados HOG muy parecido al que ya hemos estudiado, tomado en una región de imagen del tamaño indicado por la escala del punto y rotado en la dirección dominante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV proporciona una implementación de SIFT. Para calcular a la vez los puntos y los descriptores usamos `detectAndCompute`. Los puntos son estructuras con los campos que se muestran en el siguiente trozo de código, y los descriptores se devuelven como filas de un array. El número de puntos y su intensidad se controla con los argumentos de [SIFT_create](https://docs.opencv.org/4.x/d7/d60/classcv_1_1SIFT.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv.SIFT_create(nfeatures=0, contrastThreshold = 0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = readrgb('pano/pano001.jpg')\n",
    "\n",
    "kp,desc = sift.detectAndCompute(img, mask=None)\n",
    "\n",
    "# posición\n",
    "xs = [ p.pt[0] for p in kp]\n",
    "ys = [ p.pt[1] for p in kp]\n",
    "# tamaño\n",
    "sc = [ p.size  for p in kp]\n",
    "# orientación\n",
    "rs = [ p.angle for p in kp]\n",
    "\n",
    "print(len(kp))\n",
    "print(desc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para mostrar un keypoint en matplotlib, con su tamaño y orientación\n",
    "def shkeypoint(k):\n",
    "    t = np.linspace(k.angle,360+k.angle,36)*np.pi/180\n",
    "    xs = k.pt[0]+k.size*np.cos(t)\n",
    "    ys = k.pt[1]+k.size*np.sin(t)\n",
    "    plt.plot(np.append(k.pt[0],xs),np.append(k.pt[1],ys))\n",
    "\n",
    "fig(12,8)\n",
    "plt.imshow(img); ax = plt.axis();\n",
    "for k in kp:\n",
    "    if k.size > 0:\n",
    "        shkeypoint(k)\n",
    "plt.axis(ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = readrgb('sift/models/vermeer_1.jpg')\n",
    "\n",
    "kp,desc = sift.detectAndCompute(img, mask=None)\n",
    "\n",
    "fig(16,16)\n",
    "plt.imshow(img); ax = plt.axis();\n",
    "for k in kp:\n",
    "    if k.size > 10:\n",
    "        shkeypoint(k)\n",
    "plt.axis(ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos los puntos SIFT de la imagen de círculos anterior. Esta vez usamos la utilidad que proporciona OpenCV para dibujar los puntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circ = rgb2gray(cv.resize(readrgb('circles.jpg'),(0,0), fx=0.1, fy=0.1))\n",
    "circ = 255-cv.cvtColor(np.pad(circ,100,mode='constant',constant_values=255), cv.COLOR_GRAY2BGR)\n",
    "\n",
    "kp, desc = sift.detectAndCompute(circ, mask=None)\n",
    "flag = cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n",
    "cv.drawKeypoints(circ,kp,circ, color=(255,0,0), flags=flag)\n",
    "\n",
    "fig(6,6)\n",
    "print(len(kp))\n",
    "plt.imshow(circ,'gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece que el `.size` calculado se refiere al $\\sigma$ óptimo del blob, no al radio del \"círculo equivalente\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El vídeo siguiente es una captura del ejemplo de código [`sift.py`](../code/SIFT/sift.py). Cuando aparece la [calculadora](http://mycalcdb.free.fr/main.php?l=0&id=295) podemos observar que los puntos tienen a mantener un tamaño coherente con el del objeto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video controls src=\"https://github.com/albertoruiz/umucv/raw/refs/heads/master/images/demos/sift/sift.mp4\" style=\"border:0\"></video> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image matching using keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el ejemplo de código anterior, cuando pulsamos la tecla 'c' guardamos la imagen como modelo y a partir de ahí utilizamos los descriptores de los puntos de interés para intentar encontrar coindicencias:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video controls src=\"https://github.com/albertoruiz/umucv/raw/refs/heads/master/images/demos/sift/matching1.mp4\" style=\"border:0\"></video> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los \"matching\" entre puntos se obtienen cada vez, partiendo de cero, sin realizar ningún tipo de \"tracking\". El modelo puede desaparecer de la escena y cuando aparece de nuevo se recuperan las asociaciones de puntos. Esto no ocurría así con las trayectorias de \"corners\" obtenidas mediante el flujo óptico de Lucas-Kanade: cuando se interrumpen ya no pueden asociarse fácilmente en los frames siguientes. El descriptor SIFT utilizado para caracterizar los puntos permite identificarlos en imágenes nuevas de diferentes tamaños y orientaciones, resistiendo también pequeñas inclinaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video controls src=\"https://github.com/albertoruiz/umucv/raw/refs/heads/master/images/demos/sift/matching2.mp4\" style=\"border:0\"></video> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas correspondencias son erróneas. Más adelante veremos que hay métodos para eliminarlas. Lo importante es que la proporción de coincidencias respecto al número total de puntos de interés indica la apareción del modelo en la imagen. Este método supuso una revolución en la visión artificial. Hasta aquel momento el ataque más utilizado para reconocer objetos se basaba en una \"segmentación\" explícita del objeto. Los puntos de interés son una alternativa directa, basada en propiedades estadísticas de la imagen, sin segmentación previa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una aplicación directa es el reconocimiento de objetos: la similitud de dos imágenes se puede medir con el número de keypoints coincidentes (que tienen descriptores muy parecidos).\n",
    "\n",
    "La carpeta sift/models contiene tres imágenes de buena calidad de cuadros de Vermeer. En la carpeta sift/tests hay fragmentos de esas obras tomadas de cualquier manera con la webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def readfiles(path):\n",
    "    return [readrgb(file) for file in sorted(glob.glob('../images/'+path))]\n",
    "\n",
    "imgs = readfiles('sift/tests/*.*')\n",
    "mods = readfiles('sift/models/*.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(imgs[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos los puntos SIFT de todas las imágenes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv.SIFT_create()\n",
    "\n",
    "dis = [sift.detectAndCompute(x,None) for x in imgs]\n",
    "dms = [sift.detectAndCompute(x,None) for x in mods]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las coincidencias de keypoints entre dos imágenes se calculan con un [\"matcher\"](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_matcher/py_matcher.html) y luego se filtran para dejar solo las que que no tengan ambigüedad. Una forma de hacer esto es el *ratio test*: se descartan los puntos cuya mejor coincidencia es parecida a la segunda mejor. Para conseguir esta información usamos el método *knn* (k-*nearest neigbors*) del *matcher*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# por fuerza bruta\n",
    "bf = cv.BFMatcher()\n",
    "\n",
    "\n",
    "# fast approximate nearest neighbor\n",
    "# pip install opencv-contrib-python\n",
    "\n",
    "#FLANN_INDEX_KDTREE = 0\n",
    "#index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "#search_params = dict(checks=50)   # or pass empty dictionary\n",
    "#flann = cv.FlannBasedMatcher(index_params,search_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambiando el modelo `m` y la imagen de prueba `t` podemos comprobar que el número de puntos coincidentes o su proporción puede servir para reconocer objetos con suficiente textura. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(m,t):\n",
    "    print('modelo: {},  imagen: {}'.format(len(dms[m][1]), len(dis[t][1])))\n",
    "\n",
    "    fig(12,8)\n",
    "    plt.subplot(1,2,1); plt.imshow(mods[m]);\n",
    "    plt.subplot(1,2,2); plt.imshow(imgs[t]);\n",
    "\n",
    "    #                     query      model    num de vecinos\n",
    "    matches = bf.knnMatch(dis[t][1],dms[m][1],k=2)\n",
    "    #matches = flann.knnMatch(dis[t][1],dms[m][1],k=2)    \n",
    "\n",
    "    # ratio test\n",
    "    good = []\n",
    "    for mt in matches:\n",
    "        if len(mt) == 2:\n",
    "            best, second = mt\n",
    "            if best.distance < 0.75*second.distance:\n",
    "                good.append(best)        \n",
    "\n",
    "\n",
    "    print('coincidencias: {} ({:.1f}%)'.format(len(good),100*len(good)/len(dis[t][1])))\n",
    "\n",
    "\n",
    "    return good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 0\n",
    "t = 1\n",
    "\n",
    "good = compare(m,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los *matchings* contienen el grado de coincidencia y los índices de los puntos de interés de la imagen desconocida y del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching = good[0]\n",
    "\n",
    "matching.distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching.queryIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching.trainIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 0\n",
    "t = 0\n",
    "\n",
    "good = compare(m,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `cv.drawMatches` representa gráficamente las posiciones de los puntos coincidentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig(12,8)\n",
    "plt.imshow(cv.drawMatches(imgs[t], dis[t][0], mods[m], dms[m][0], good,\n",
    "                      flags=0,\n",
    "                      matchColor=(128,255,128),\n",
    "                      singlePointColor = (128,128,128),\n",
    "                      outImg=None) );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otros *Keypoints* disponibles en OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV tiene muchos otros detectores de *keypoints*, que suelen utilizarse con un interfaz común."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = readrgb('pano/pano001.jpg')\n",
    "\n",
    "#method = cv.xfeatures2d.StarDetector_create()\n",
    "#method = cv.FastFeatureDetector_create()\n",
    "#method = cv.ORB_create()\n",
    "#method = cv.xfeatures2d.SIFT_create()\n",
    "#method = cv.xfeatures2d.SURF_create()\n",
    "method = cv.AKAZE_create()\n",
    "\n",
    "kp = method.detect(img)\n",
    "\n",
    "xs = [ p.pt[0] for p in kp]\n",
    "ys = [ p.pt[1] for p in kp]\n",
    "\n",
    "fig(12,8)\n",
    "plt.imshow(img); ax = plt.axis('off');\n",
    "plt.plot(xs,ys,'.r'); plt.axis(ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependiendo de las necesidades de nuestra aplicación (velocidad de cálculo vs estabilidad de los puntos) elegiremos el más adecuado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Bag of visual words*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando hay muchos modelos la búsqueda de coincidencias en todos ellos puede ser muy costosa. Una forma de acelerar el proceso es construir un \"[vocabulario visual](https://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision)\" a partir de una gran colección de keypoints extraídos de los modelos disponibles o de escenas parecidas (mediante [k-means](https://en.wikipedia.org/wiki/K-means_clustering) o algún método parecido). Cada imagen se representa mediante un histograma que contabiliza el número de ocurrencias de cada \"visual word\". Este histograma sirve como *feature* para comparar las imágenes o para entrenar un clasificador. ([Csurka 2004](https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/csurka-eccv-04.pdf))."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "nbTranslate": {
   "displayLangs": [
    "en",
    "es"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "es",
   "targetLang": "en",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "state": {
    "2307db0288c6461cacd6e3219a065fa4": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "580ca9ec321c46ad8fb2472a8614be09": {
     "views": [
      {
       "cell_index": 31
      }
     ]
    },
    "694afa53a82148e8a108c71ca061d685": {
     "views": [
      {
       "cell_index": 32
      }
     ]
    },
    "c80fe96f6027407b9804fadc5941c1b7": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sesiones prácticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalación de Python + ecosistema científico + opencv + opengl\n",
    "\n",
    "- aula virtual -> página web -> install\n",
    "- git o unzip master\n",
    "- anaconda completo o miniconda\n",
    "- linux, windows, mac\n",
    "- probar scripts con webcam y verificar opengl, dlib etc.\n",
    "- manejo básico de jupyter\n",
    "- repaso Python\n",
    "- Ejercicio: recortar y unir imágenes para conseguir [algo como esto](../images/demos/ej-c0.png).\n",
    "\n",
    "Opcional:\n",
    "\n",
    "- compilación opencv\n",
    "- docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio de comprobación de FOV/tamaños/distancias.\n",
    "\n",
    "Dispositivos de captura\n",
    "\n",
    "- umucv (install con --upgrade) (update_umucv.sh)\n",
    "- webcam.py con opencv crudo\n",
    "- stream.py, opciones de autostream, efecto de teclas, --help, --dev=help\n",
    "\n",
    "    - webcams\n",
    "    - videos\n",
    "    - carpeta de imágenes\n",
    "    - teléfono\n",
    "    - youtube\n",
    "    - urls de tv\n",
    "        \n",
    "- ejemplo de recorte invertido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Más utilidades\n",
    "\n",
    "- spyder\n",
    "\n",
    "- PYTHONPATH\n",
    "\n",
    "- control de webcam v4l2-ctl, vlc, gucview\n",
    "\n",
    "- wzoom.py (para las ventanas de Windows que no tienen zoom)\n",
    "\n",
    "- help_window.py\n",
    "\n",
    "- save_video.py\n",
    "\n",
    "- ampliar mouse.py:\n",
    "\n",
    "    - círculos en las posiciones marcadas (cv.circle)\n",
    "    \n",
    "    - coordenadas textuales (cv.putText  (ej. en hello.py) o umucv.util.putText)\n",
    "\n",
    "    - marcar solo los dos últimos (pista: collections.deque)\n",
    "\n",
    "    - reproducir code/medidor.py indicando la distancia en pixels\n",
    "\n",
    "    - Dado el FOV: indicar el ángulo de las direcciones marcadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deque.py\n",
    "\n",
    "roi.py:\n",
    "\n",
    "- añadir la media del nivel de gris del recorte\n",
    "\n",
    "- guardar el recorte y mostrar cv.absdiff respecto al frame actual, mostrando su media o su máximo.\n",
    "\n",
    "(Sirve de punto de partida para el ejercicio ACTIVIDAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- aclaraciones ejercicio COLOR\n",
    "\n",
    "- demo spectral\n",
    "\n",
    "- trackbar.py\n",
    "\n",
    "- demo filtros\n",
    "\n",
    "Ejercicio: \n",
    "\n",
    "- implementación de filtro gaussiano con tracker sigma en toda la imagen, monocromo ([ejemplo](../images/demos/ej-c4-0.png)).\n",
    "- add box y median\n",
    "- medir y mostrar tiempos de cómputo en diferentes casos\n",
    "\n",
    "(Sirve de punto de partida para el ejercicio opcional FILTROS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HOG\n",
    "\n",
    "- (captura asíncrona)\n",
    "\n",
    "- (teoría de HOG, implementación sencilla)\n",
    "\n",
    "- hog0.py en detalle\n",
    "\n",
    "- pedestrian.py, detección multiescala\n",
    "\n",
    "- DLIB facelandmarks.py: HOG face detector con landmarks\n",
    "\n",
    "Ejercicio: blink detection, inpainting eyes, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detección de corners y Flujo óptico de Lucas-Kanade\n",
    "\n",
    "- LK/*.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a construir un \"tracker\" de puntos de interés basado en el método de Lucas-Kanade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer paso es construir un detector de corners partiendo de cero, calculando la imagen de respuesta correspondiente al menor valor propio de la matriz de covarianza de la distribución local del gradiente en cada pixel (`corners0.py`). En realidad esta operación está directamente disponible en opencv mediante cv.goodFeaturesToTrack (`corners1.py`, `corners2.py )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente ejemplo muestra cómo encontrar directamente con `cv.calcOpticalFlowPyrLK` la posición de los puntos detectados en el fotograma siguiente, sin necesidad de recalcular puntos nuevos y asociarlos con los anteriores (`lk_track0.py`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación ampliamos el código para generar puntos nuevos periódicamente y crear una lista de trayectorias \"tracks\" que se mantiene actualizada en cada fotograma (`lk_track1.py`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, ampliamos el código anterior para que solo se generen puntos nuevos en zonas de la imagen donde no los haya, y mejoramos la detección de las posiciones siguientes con un criterio de calidad muy robusto que exige que la predicción hacia el pasado de los puntos nuevos coincida con el punto inicial. Si no hay una asociación mutua el punto y su trayectoria se descartan (`lk_tracks.py`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicios:\n",
    "\n",
    "- Analizar los tracks para determinar en qué dirección se mueve la cámara (UP,DOWN,LEFT,RIGHT, [FORWARD, BACKWARD])\n",
    "\n",
    "- Estudiar la posibilidad de hacer tracking de un ROI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimentamos con el detector de puntos de interés SIFT.\n",
    "\n",
    "Nuestro objetivo es obtener un conjunto de \"keypoints\", cada uno con su descriptor (vector de características que describe el entorno del punto), que permita encontrarlo en imágenes futuras. Esto tiene una aplicación inmediata para reconocer objetos y más adelante en geometría visual.\n",
    "\n",
    "Empezamos con el ejemplo de código code/SIFT/sift0.py, que simplemente calcula y muestra los puntos de interés. Es interesante observar el efecto de los parámetros del método y el tiempo de cómputo en función del tamaño de la imagen (que puedes cambiar con --size o --resize).\n",
    "\n",
    "El siguiente ejemplo code/SIFT/sift1.py muestra un primer ataque para establecer correspondencias. Los resultados son bastante pobres porque se aceptan todas las posibles coincidencias.\n",
    "\n",
    "Finalmente, en code/SIFT/sift.py aplicamos un criterio de selección para eliminar muchas correspondencias erróneas (aunque no todas). Esto es en principio suficiente para el reconocimiento de objetos. (Más adelante veremos una forma mucho mejor de eliminar correspondencias erróneas, necesaria para aplicaciones de geometría.)\n",
    "\n",
    "El ejercicio obligatorio **SIFT** es una ampliación sencilla de este código. Se trata de almacenar un conjunto de modelos (¡con textura! para que tengan suficientes keypoints) como portadas de libros, discos, videojuegos, etc. y reconocerlos en base a la proporción de coincidencias detectadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la segunda parte de la clase experimentamos con un servidor mjpg y creamos bots de telegram (explicados al final de este documento) para comunicarnos fácilmente con las aplicaciones de visión artificial desde el móvil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconocimiento de formas mediante descriptores frecuenciales.\n",
    "\n",
    "Nuestro objetivo es hacer un programa que reconozca la forma de trébol, como se muestra [en este pantallazo](../images/demos/shapedetect.png). Si no tenéis a mano un juego de cartas podéis usar --dev=dir:../images/card*.png para hacer las pruebas, aunque lo ideal es hacerlo funcionar con una cámara en vivo.\n",
    "\n",
    "Trabajaremos con los ejemplos de código de la carpeta `code/shapes` y, como es habitual, iremos añadiendo poco a poco funcionalidad. En cada nuevo paso los comentarios explican los cambios respecto al paso anterior.\n",
    "\n",
    "Empezamos con el ejemplo shapes/trebol1.py, que simplemente prepara un bucle de captura básico, binariza la imagen y muestra los contornos encontrados. Se muestran varias formas de realizar la binarización y se puede experimentar con ellas, pero en principio el método automático propuesto suele funcionar bien en muchos casos.\n",
    "\n",
    "El segundo paso en shapes/trebol2.py junta la visualización en una ventana y selecciona los contornos oscuros de tamaño razonable. Esto no es imprescincible para nuestra aplicación, pero es conveniente familiarizarse con el concepto de orientación de un contorno.\n",
    "\n",
    "En shapes/trebol3.py leemos un modelo de la silueta trébol de una imagen que tenemos en el repositorio y la mostramos en una ventana.\n",
    "\n",
    "En shapes/trebol3b.py hacemos una utilidad para ver gráficamente las componentes frecuenciales como elipses que componen la figura. Podemos ver las componentes en su tamaño natural, incluyendo la frecuencia principal, [como aquí](../images/demos/full-components.png), o quitando la frecuencia principal y ampliando el tamaño de las siguientes, que son la base del descriptor de forma, [como se ve aquí](../images/demos/shape-components.png). Observa que las configuraciones de elipses son parecidas cuando corresponden a la misma silueta.\n",
    "\n",
    "En shapes/trebol4.py definimos la función que calcula el descriptor invariante. Se basa esencialmente en calcular los tamaños relativos de estas elipses. En el código se explica cómo se consigue la invarianza a las transformaciones deseadas: posición, tamaño, giros, punto de partida del contorno y ruido de medida.\n",
    "\n",
    "Finalmente, en shapes/trebol5.py calculamos el descriptor del modelo y en el bucle de captura calculamos los descriptores de los contornos oscuros detectados para marcar las siluetas que tienen un descriptor muy parecido al del trébol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta subsesión vamos a hacer varias actividades. Necesitamos algunos paquetes. En Linux son:\n",
    "\n",
    "    sudo apt install tesseract-ocr tesseract-ocr-spa libtesseract-dev\n",
    "    pip install tesserocr\n",
    "\n",
    "    sudo apt install libzbar-dev\n",
    "    pip install pyzbar\n",
    "\n",
    "[zbar Windows instaler](http://zbar.sourceforge.net) -> download\n",
    "\n",
    "Usuarios de Mac y Windows: investigad la forma de instalar tesseract.\n",
    "\n",
    "\n",
    "1) En primer lugar nos fijamos en el script [code/ocr.py](../code/ocr.py), cuya misión es poner en marcha el OCR con la cámara en vivo. Usamos el paquete de python `tesserocr`. Vamos a verificar el funcionamiento con una imagen estática, pero lo ideal es probarlo con la cámara en vivo.\n",
    "\n",
    "    ./ocr.py --dev=dir:../images/texto/bo0.png \n",
    "\n",
    "Está pensado para marcar una sola línea de texto, [como se muestra aquí](../images/demos/ocr.png). Este pantallazo se ha hecho con la imagen bo1.png disponible en la misma carpeta, que está desenfocada, pero aún así el OCR funciona bien.\n",
    "\n",
    "(En windows parece que hay que usar pytesseract en lugar de tesserocr, lo que requiere adaptar del script.)\n",
    "\n",
    "Para mostrar la complejidad de un ocr mostramos el resultado del script `crosscorr.py` sobre images/texto.png, para observar que la comparación pixel a pixel no es suficiente para obtener resultados satisfactorios. En esa misma imagen la binarización y extracción de componentes conexas no consigue separar letras individuales.\n",
    "\n",
    "Finalmente demostramos mediante `spectral.py` que la transormada de Fourier 2D permite detectar el ángulo y la separación entre renglones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) El segundo ejemplo es `code/zbardemo.png` que muestra el uso del paquete pyzbar para leer códigos de barras ([ejemplo](../images/demos/barcode.png)) y códigos QR ([ejemplo](../images/demos/qr.png)) con la cámara. En los códigos de barras se detectan puntos de referencia, y en los QR se detectan las 4 esquinas del cuadrado, que pueden ser útiles como referencia en algunas aplicaciones de geometría."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) demo de `grabcut.py` para segmentar interactivamente una imagen. Lo probamos con images/puzzle3.png."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Ponemos en marcha el detector de caras de opencv con la webcam en vivo y comparamos con el detector de DLIB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoy vamos a rectificar el plano de la mesa apoyándonos en marcadores artificiales.\n",
    "\n",
    "En primer lugar trabajaremos con marcadores poligonales. Nuestro objetivo es detectar un marcador como el que aparece en el vídeo `images/rot4.mjpg`. Nos vamos a la carpeta `code/polygon`.\n",
    "\n",
    "El primer paso (`polygon0.py`) es detectar figuras poligonales con el número de lados correcto a partir de los contornos detectados.\n",
    "\n",
    "A continuación (`polygon1.py`) nos quedamos con los polígonos que realmente pueden corresponder al marcador. Esto se hace viendo si existe una homografía que relaciona con precisión suficiente el marcador real y su posible imagen.\n",
    "\n",
    "Finalmente (`polygon2.py`) obtiene el plano rectificado\n",
    "\n",
    "También se puede añadir información \"virtual\" a la imagen original, como por ejemplo los ejes de coordenadas definidos por el marcador (`polygon3.py`).\n",
    "\n",
    "\n",
    "Como segunda actividad, en la carpeta `code/elipses` se muestra la forma de detectar un marcador basado en 4 círculos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sesión vamos a extraer la matriz de cámara a partir del marcador utilizado en la sesión anterior, lo que nos permitirá añadir objetos virtuales tridimensionales a la escena y determinar la posición de la cámara en el espacio.\n",
    "\n",
    "Nos vamos a la carpeta `code/pose`, donde encontraremos los siguientes ejemplos de código:\n",
    "\n",
    "`pose0.py` incluye el código completo para extraer contornos, detectar el marcador poligonal, extraer la matriz de cámara y dibujar un cubo encima del marcador.\n",
    "\n",
    "`pose1.py` hace lo mismo con funciones de umucv.\n",
    "\n",
    "`pose2.py` trata de ocultar el marcador y dibuja un objeto que cambia de tamaño.\n",
    "\n",
    "`pose3.py` explica la forma de proyectar una imagen en la escena escapando del plano del marcador.\n",
    "\n",
    "`pose3D.py` es un ejemplo un poco más avanzado que utiliza el paquete pyqtgraph para mostrar en 3D la posición de la cámara en el espacio.\n",
    "\n",
    "En el ejercicio **RA** puedes intentar que el comportamiento del objeto virtual dependa de acciones del usuario (p. ej. señalando con el ratón un punto del plano) o de objetos que se encuentran en la escena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breve introducción a scikit-learn y keras.\n",
    "\n",
    "En primer lugar repasaremos algunos conceptos básicos en el notebook [machine learning](machine-learning.ipynb).\n",
    "\n",
    "Esta sesión está dedicada a poner en marcha una red convolucional sencilla. La tarea que vamos a resolver es el reconocimiento de dígitos manuscritos. Por eso, en primer lugar es conveniente escribir unos cuantos números en una hoja de papel, con un bolígrafo que tenga un trazo no demasiado fino, y sin preocuparnos mucho de que estén bien escritos. Pueden tener distintos tamaños, pero no deben estar muy girados. Para desarrollar el programa y hacer pruebas cómodamente se puede trabajar con una imagen fija, pero la idea es que nuestro programa funcione con la cámara en vivo.\n",
    "\n",
    "Trabajaremos en la carpeta [code/DL/CNN](../code/DL/CNN), donde tenemos las diferentes etapas de ejercicio y una imagen de prueba.\n",
    "\n",
    "El primer paso es `digitslive-1.py` que simplemente encuentra las manchas de tinta que pueden ser posibles números.\n",
    "\n",
    "En `digitslive-2.py` normalizamos el tamaño de las detecciones para poder utilizar la base de datos MNIST.\n",
    "\n",
    "En `digitslive-3.py` implementamos un clasificador gaussiano con reducción de dimensión mediante PCA y lo ponemos en marcha con la imagen en vivo. (Funciona bastante bien pero, p.ej., en la imagen de prueba comete un error).\n",
    "\n",
    "Finalmente, en `digitslive-4.py` implementamos la clasificación mediante una red convolucional mediante el paquete **keras**. Usamos unos pesos precalculados. (Esta máquina ya no comete el error anterior.)\n",
    "\n",
    "Como siempre, en cada fase del ejercicio los comentarios explican el código que se va añadiendo.\n",
    "\n",
    "Una vez conseguido esto, la sesión práctica tiene una segunda actividad que consiste en **entrenar los pesos** de (por ejemplo) esta misma red convolucional. Para hacerlo en nuestro ordenador sin perder la paciencia necesitamos una GPU con CUDA y libCUDNN. La instalación de todo lo necesario puede no ser trivial. \n",
    "\n",
    "Una alternativa muy práctica es usar [google colab](https://colab.research.google.com/), que proporciona gratuitamente máquinas virtuales con GPU y un entorno de notebooks jupyter (un poco modificados pero compatibles). Para probarlo, entrad con vuestra cuenta de google y abrid un nuevo notebook. En la opción de menú **Runtime** hay que seleccionar **Change runtime type** y en hardware accelerator ponéis GPU. En una celda del notebook copiáis directamente el contenido del archivo `cnntest.py` que hay en este mismo directorio donde estamos trabajando hoy. Al evaluar la celda se descargará la base de datos y se lanzará un proceso de entrenamiento. Cada epoch tarda unos 4s. Podéis comparar con lo que se consigue con la CPU en vuestro propio ordenador. Se puede lanzar un entrenamiento más completo, guardar los pesos y descargarlos a vuestra máquina.\n",
    "\n",
    "Como curiosidad, podéis comparar con lo que conseguiría el OCR tesseract, y guardar algunos casos de dígitos que estén bien dibujados pero que la red clasifique mal.\n",
    "\n",
    "Finalmente, entrenamos un autoencoder y comparamos el resultado con la reducción de dimensión PCA explicada al principio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sesión vamos a poner en marcha algunos modelos más avanzados de deep learning.\n",
    "\n",
    "Los ejemplos de código se han probado sobre LINUX. En Windows o Mac puede ser necesario hacer modificaciones; para no perder mucho tiempo mi recomendación es probarlo primero en una máquina virtual.\n",
    "\n",
    "Si tenéis una GPU nvidia reciente lo ideal es instalar CUDA y libCUDNN para conseguir una mayor velocidad de proceso. Si no tenéis GPU no hay ningún problema, todos los modelos funcionan con CPU. (Los ejercicios de deep learning que requieren entrenamiento son opcionales y se pueden entrenar en COLAB.)\n",
    "\n",
    "Para ejecutar las máquinas inception, YOLO y el reconocimiento de caras necesitamos los siguientes paquetes:\n",
    "\n",
    "    pip install  face_recognition  tensorflow==1.15.0  keras  easydict\n",
    "\n",
    "La detección de marcadores corporales *openpose* requiere unos pasos de instalación adicionales que explicaremos más adelante.\n",
    "\n",
    "(La versión 1.15.0 de tensorflow es necesaria para YOLO y openpose. Producirá algunos warnings sin mucha importancia. Si tenemos una versión más reciente de tensorflow podemos hacer `pip install --upgrade tensorflow=1.15.0` o crear un entorno de conda especial para este tema)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Para probar el **reconocimiento de caras** nos vamos a la carpeta code/DL/facerec. Debe estar correctamente instalado DLIB. \n",
    "\n",
    "En el directorio `gente` se guardan los modelos. Como ejemplo tenemos a los miembros de Monty Python:\n",
    "\n",
    "    ./facerec.py --dev=dir:../../../images/monty-python*\n",
    "    \n",
    "(Recuerda que las imágenes seleccionadas con --dev=dir: se avanzan pinchando con el ratón en la ventana pequeña de muestra).\n",
    "\n",
    "Puedes meter fotos tuyas y de tu familia en la carpeta `gente` para probar con la webcam o con otras fotos.\n",
    "\n",
    "Con pequeñas modificaciones de este programa se puede resolver el ejercicio ANON: selecciona una cara en la imagen en vivo pinchando con el ratón para ocultarla  (emborronándola o pixelizándola) cuando se reconozca en las imágenes siguientes.\n",
    "\n",
    "Esta versión del reconocimiento de caras no tiene aceleración con GPU (tal vez se puede configurar). Si reducimos un poco el tamaño de la imagen funciona con bastante fluidez."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Para probar la máquina **inception** nos movemos a la carpeta code/DL/inception.\n",
    "\n",
    "    ./inception0.py\n",
    "    \n",
    "(Se descargará el modelo del la red). Se puede probar con las fotos incluidas en la carpeta con `--dev=dir:*.png`. La versión `inception1.py` captura en hilo aparte y muestra en consola las 5 categorías más probables.\n",
    "\n",
    "Aunque se supone que consigue buenos resultados en las competiciones, sobre imágenes naturales comete bastante errores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) El funcionamiento de **YOLO** es mucho mejor. Nos vamos a la carpeta code/DL y ejecutamos lo siguiente para para descargar el código y los datos de esta máquina (y de openpose).\n",
    "\n",
    "    bash get.sh\n",
    "    \n",
    "Nos metemos en code/DL/yolo y ejecutamos:\n",
    "\n",
    "    /.yolo-v3.py\n",
    "    \n",
    "Se puede probar también con las imágenes de prueba incluidas añadiendo `--dev=dir:*.png`.\n",
    "\n",
    "El artículo de [YOLO V3](https://pjreddie.com/media/files/papers/YOLOv3.pdf) es interesante. En la sección 5 el autor explica que abandonó esta línea de investigación por razones éticas. Os recomiendo que la leáis. Como curiosidad, hace unos días apareció [YOLO V4](https://arxiv.org/abs/2004.10934)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Para probar **openpose** nos vamos a code/DL/openpose. Los archivos necesarios ya se han descargado en el paso anterior, pero necesitamos instalar algunos paquetes. El proceso se explica en el README. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la carpeta `docker` hay un script para ejecutar una imagen docker que tiene instalados todos los paquetes que hemos estamos usando en la asignatura. Es experimental. No perdaís ahora tiempo con esto si no estáis familiarizados con docker.\n",
    "\n",
    "El tema de deep learning en visión artificial es amplísimo. Para estudiarlo en detalle hace falta (como mínimo) una asignatura avanzada (master). Nuestro objetivo es familizarizarnos un poco con algunas de las máquinas preentrenadas disponibles para hacernos una idea de sus ventajas y limitaciones.\n",
    "\n",
    "Si estáis interesados en estos temas el paso siguiente es adaptar alguno de estos modelos a un problema propio mediante \"transfer learning\", que consiste en utilizar las primeras etapas de una red preentrenada para transformar nuestros datos y ajustar un clasificador sencillo. Alternativamente, se puede reajustar los pesos de un modelo preentrenado, fijando las capas iniciales al principio. Para remediar la posible falta de ejemplos se utilizan técnicas de \"data augmentation\", que generan variantes de los ejemplos de entrenamiento con múltiples transformaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## entrenar dlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (opcional) DLIB herramienta de etiquetado imglab. Entrenamiento de detector HOG SVM con herramientas de DLIB:\n",
    "\n",
    "    - descargar y descomprimir dlib source\n",
    "    - ir a los ejemplos/faces\n",
    "    - meter dentro imglab (que hay que compilar pero tenemos versión precompilada en robot/material/va)\n",
    "    - mostrar los training.xml y testing.xml (se pueden crear otros)\n",
    "    - meter dentro train_detector.py y run_detector.py de code/hog\n",
    "    - ./train_detector training.xml testing.xml  (crea detector.svm)\n",
    "    - ./run_detector detector.svm --dev=dir:\\*.jpg   (o también --dev=dir:/path/to/umucv/images/monty\\*)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correlation filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comentamos el método de detección de objetos por correlación cruzada, que es el mismo criterio que se usa para buscar la posición de *corners* en imágenes sucesivas, y luego vemos la demostración del discriminative correlation filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- crosscorr.py\n",
    "\n",
    "- dcf.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## flask server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El ejemplo `server.py` explica cómo hacer un servidor web sencillo con *flask* para enviar un pantallazo de la imagen actual de la webcam, y `mjpegserver.py` explica cómo hacer un servidor de streaming en formato mjpeg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## telegram bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a jugar con un bot de telegram que nos permite comunicarnos cómodamente con nuestro ordenador desde el teléfono móvil, sin necesidad de tener una dirección pública de internet.\n",
    "\n",
    "Simplemente necesitamos:\n",
    "\n",
    "    pip install python-telegram-bot\n",
    "\n",
    "El ejemplo `bot/bot0.py` nos envía al teléfono la IP del ordenador (es útil si necesitamos conectarnos por ssh con una máquina que tiene IP dinámica).\n",
    "\n",
    "El ejemplo `bot/bot1.py` explica la forma de enviar una imagen nuestro teléfono cuando ocurre algo. En este caso se envía cuando se pulsa una tecla, pero lo normal es detectar automáticamente algún evento con las técnicas de visión artificial que estamos estudiando.\n",
    "\n",
    "El ejemplo `bot/bot2.py` explica la forma de hacer que el bot responda a comandos. El comando /hello nos devuelve el saludo, el comando /stop detiene el programa y el comando /image nos devuelve una captura de nuestra webcam. (Se ha usado la captura en un hilo). \n",
    "\n",
    "El ejemplo `bot/bot3.py` explica la forma de capturar comandos con argumentos y el procesamiento de una imagen enviada por el usuario.\n",
    "\n",
    "Esta práctica es muy útil para enviar cómodamente a nuestros programas de visión artificial una imagen tomada con la cámara sin necesidad de escribir una aplicación específica para el móvil. Algunos ejercicios que estamos haciendo se pueden adaptar fácilmente para probarlos a través de un bot de este tipo.\n",
    "\n",
    "Para crearos vuestro propio bot tenéis que contactar con el bot de telegram \"BotFather\", que os guiará paso a paso y os dará el token de acceso. Y luego el \"IDBot\" os dirá el id numérico de vuestro usuario."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nbTranslate": {
   "displayLangs": [
    "en",
    "es"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "es",
   "targetLang": "en",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

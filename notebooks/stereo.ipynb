{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de la geometría visual es la \"reconstrucción\" de una escena 3D completamente desconocida a partir de varias imágenes.\n",
    "\n",
    "En los capítulos anteriores hemos trabajado en la situación muy restrictiva de escenas planas que contienen marcadores de referencia artificiales. Con una única imagen somos capaces de rectificar el plano, introducir objetos virtuales y determinar la localización de la cámara."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta lección presenta los fundamentos de la geometría de dos vistas (visión estéreo). Una exposición más detallada se puede encontrar en mis [antiguos apuntes](https://robot.inf.um.es/material/va/percep.pdf#chapter.7). \n",
    "\n",
    "Hasta ahora sabemos que dos vistas de un plano (o de una escena 3D si la cámara no se desplaza) están relacionadas por una homografía. Pero si escena es 3D y la cámara se desplaza aparece el **paralaje** (los objetos cambian más o menos de posición dependiendo de la profundidad) y ya no se puede transformar una imagen en otra. Aún así, existe una relación entre las dos imágenes (más débil que una sencilla homografía) llamada **\"restricción epipolar\"** en la que podemos apoyarnos para reconstruir la escena.\n",
    "\n",
    "El concepto esencial que aprenderemos es:\n",
    "\n",
    "- Si identificamos varios puntos en dos imágenes de una misma escena podemos determinar (salvo escala) la posición de las cámaras y de los puntos 3D.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Depth map*\n",
    "\n",
    "Antes de empezar es importante recordar que las cámaras RGBD (Red,Green,Blue,Depth) como la [kinect](https://en.wikipedia.org/wiki/Kinect) incluyen un canal de profundidad que permite obtener directamente información 3D sin necesidad de utilizar visión estereoscópica.\n",
    "\n",
    "\n",
    "<img src=\"../images/demos/xtion.jpg\" width=\"60%\"/>\n",
    "\n",
    "OpenCV proporciona herramientas para la [manipulación de imágenes RGBD](https://docs.opencv.org/3.4/d2/d3a/group__rgbd.html).\n",
    "\n",
    "Cuando nos enfrentamos a una nueva aplicación debemos valorar si este tipo de dispositivos es adecuado para las condiciones de trabajo, o si necesitamos recurrir a la geometría de múltiples vistas que estudiaremos a continuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelos recientes de Deep Learning basados en Visual Transformers como por ejemplo [Dino V2](https://dinov2.metademolab.com/demos?category=depth) consiguen mapas de profundidad a partir de una única imagen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visión estereoscópica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La figura siguiente resume la situación a la que nos enfrentamos. Tenemos dos vistas de una escena, tomadas por dos cámaras distintas o por una sola cámara que se mueve. Suponemos que la escena observada no cambia.\n",
    "\n",
    "\n",
    "Cada punto del espacio $P=(X,Y,Z)$ se proyecta en la posición $p_1=(x_1,y_1)$ en la imagen izquierda y en $p_2=(x_2,y_2)$ en la imagen derecha.\n",
    "\n",
    "<img src=\"../images/demos/stereo.png\" width=\"50%\"/>\n",
    "\n",
    "Si tenemos las dos matrices de cámara $\\mathsf M_1$ y $\\mathsf M_2$ es inmediato calcular las imágenes de un punto $P$ conocido:  $p_1 = \\mathsf M_1 P$ y $p_2 = \\mathsf M_2 P$ (por supuesto, usando coordenadas homogéneas).\n",
    "\n",
    "Y a la inversa, si lo que conocemos son las proyecciones $p_1$ y $p_2$, asumiendo que corresponden al mismo punto $P$ del espacio, podemos calcular sus coordenadas mediante la intersección de los rayos ópticos. Esta operación se llama **triangulación**.\n",
    "\n",
    "¿Qué ocurre si desconocemos las matrices de cámara? Una posibilidad es introducir un marcador en la escena para estimarlas como hicimos en el capítulo anterior. Como veremos a continuación, esto no es necesario: las cámaras pueden recuperarse en una escena completamente desconocida si somos capaces de identificar varios puntos coincidentes en ambas imágenes.\n",
    "\n",
    "Antes de empezar, es interesante echar un vistazo a los conceptos de\n",
    "\n",
    "- [stellar parallax](https://en.wikipedia.org/wiki/Stellar_parallax): La distancia de objetos en el espacio exterior se mide mediante triangulación.\n",
    "\n",
    "- [disparity](https://docs.opencv.org/4.7.0/dd/d53/tutorial_py_depthmap.html#gsc.tab=0): La geometría elemental de la triangulación.\n",
    "\n",
    "- [random dot stereogram](https://en.wikipedia.org/wiki/Random_dot_stereogram), [autostereogram](https://en.wikipedia.org/wiki/Autostereogram): La percepción de profundidad en los humanos es un proceso de bajo nivel que se efectúa antes del reconocimiento de objetos.\n",
    "\n",
    "- Un artículo sobre la [tecnología de reconstrucción 3D de Google](https://www.trekview.org/blog/2019/google-street-view-cameras-more-than-meets-the-eye/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproyección de puntos y rectas\n",
    "\n",
    "El conjunto de puntos del espacio que pueden haber generado un punto de la imagen es una recta (el rayo óptico):\n",
    "\n",
    "<img src=\"../images/demos/repropoint.png\" width=\"30%\"/>\n",
    "\n",
    "El conjunto de de puntos del espacio que pueden haber generado una recta de la imagen es un plano:\n",
    "\n",
    "\n",
    "<img src=\"../images/demos/reproline.png\" width=\"30%\"/>\n",
    "\n",
    "\n",
    "Si tenemos la matriz de cámara podemos obtener expresiones (sencillas si usamos el formalismo matemático adecuado) para la recta o el plano reproyectados.\n",
    "\n",
    "### Triangulación\n",
    "\n",
    "Como hemos comentado más arriba, el punto 3D se consigue mediante la intersección de los rayos ópticos. Ahora bien, dos rectas en el espacio no tienen por qué tocarse. Si las proyecciones $p_1$ y $p_2$ realmente no corresponden al mismo punto obviamente la triangulación fallará. En realidad siempre fallará con datos reales debido al ruido de medida aunque la correspondencia sea correcta. Por esta razón, los algoritmos de triangulación producen soluciones de compromiso como p. ej. el punto más cercano a las dos rectas.\n",
    "\n",
    "<img src=\"../images/demos/triangulation2.png\" width=\"50%\"/>\n",
    "\n",
    "(Existen \"cámaras estéreo\" que tienen las dos cámaras perfectamente alineadas y precalibradas de tal manera que los puntos correspondientes siempre están en la misma fila y el proceso de triangulación se simplifica mucho. Nosotros continuamos analizando el caso general.)\n",
    "\n",
    "En definitiva, tenemos el\n",
    "\n",
    "**Resultado 1**: Si conocemos las matrices de cámara podemos triangular puntos correspondientes.\n",
    "\n",
    "En este momento nos damos cuenta de un **detalle muy importante**:\n",
    "\n",
    "$$2+2>3$$\n",
    "\n",
    "¿Qué queremos decir con esto? Que un punto del espacio, que tiene 3 grados de libertad (las coordenadas cartesianas $(X,Y,Z)$) está \"codificado\" o representado por las dos proyecciones $p_1$ y $p_2$, cada una con 2 grados de libertad, lo que supone un conjunto de 4 números ($x_1, y_1, x_2, y_2$). Por tanto, hay redundancia. Para recuperar el punto 3D en principio bastaría con 3 coordenadas de imagen. (Es la razón de que los rayos en general no se toquen.) Por supuesto, nos viene muy bien tener esta información adicional, gracias a la cual podemos hacer una estimación mejor en presencia de ruido.\n",
    "\n",
    "Este grado de libertad adicional es más importante de lo que parece: vamos a explotarlo para deducir la posición de las cámaras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometría epipolar\n",
    "\n",
    "Imagina que el rayo óptico (en rojo) del punto $x$ (antes lo hemos llamado $p_1$) en la cámara $\\mathsf M$ fuera visible en el espacio. Se vería como una recta $l'$ en la otra cámara $\\mathsf N$. La imagen $x'$ (antes $p_2$) del punto 3D en esta segunda cámara debe estar necesariamente en esa recta $l'$. Esta recta se llama **recta epipolar**.\n",
    "\n",
    "<img src=\"../images/demos/epipolar.png\" width=\"35%\"/>\n",
    "\n",
    "Realmente las dos proyecciones solo tienen 3 grados de libertad: el punto derecho solo puede moverse por la recta epipolar, dependiendo de la distancia a la que se encuentre dentro del rayo óptico del punto izquierdo. Los valores que pueden tomar ($x_1, y_1, x_2, y_2$) están sujetos a la condición $l' \\cdot x' = 0$, que se llama \"epipolar constraint\".\n",
    "\n",
    "\n",
    "### Restricción epipolar\n",
    "\n",
    "Si conocemos las matrices de cámara se pueden calcular las recta epipolares de cualquier punto. La forma de hacer esto es muy simple: proyectamos en la segunda imagen dos puntos cualesquiera del rayo óptico y los unimos. Se puede demostrar que esta operación es lineal (con coordenadas homogéneas, por supuesto) y por tanto se reduce a la multiplicación por una matriz $(3\\times 3)$. Es la **matriz fundamental** $\\mathsf F$ de la pareja de cámaras. Por tanto $l' = \\mathsf F x$, donde $\\mathsf F$ depende de $\\mathsf M_1$ y $\\mathsf M_2$.\n",
    "\n",
    "**Resultado 2**: La relación entre las dos cámaras se condensa en la matriz fundamental $\\mathsf F$, que caracteriza la restricción epipolar:\n",
    "\n",
    "$$x' \\cdot \\mathsf F x = 0$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Resultado 3**: La restricción epipolar sirve para descartar correspondencias erróneas.\n",
    "\n",
    "Por ejemplo, en la figura siguiente, queremos saber cuál es el punto en la imagen derecha que corresponde con el punto azul que se ve en la imagen izquierda. Hay 4 posibilidades. Los puntos verdes claramente no pueden ser, su color no coincide. De los dos azules podemos descartar el de arriba ya que no cumple la restricción epipolar. \n",
    "\n",
    "<img src=\"../images/demos/epipolar2.png\" width=\"50%\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstrucción estéreo\n",
    "\n",
    "La matriz $\\mathsf F$ que \"implementa\" la restricción epipolar se puede deducir de un conjunto de puntos correspondientes. La condición $x' \\cdot \\mathsf F x = 0$ es lineal en los elementos de $\\mathsf F$, por lo que podemos encontrar la matriz fundamental resolviendo un sistema de ecuaciones homogéneo (ver [DLT.ipynb](DLT.ipynb)). En la práctica es preferible utilizar la función especializada `cv.findFundamentalMat` que tiene en cuenta que las matrices $\\mathsf F$ físicamente posibles tienen rango 2. En definitiva:\n",
    "\n",
    "\n",
    "**Resultado 4**: Unos pocos puntos correspondientes permiten obtener directamente la geometría epipolar.\n",
    "\n",
    "---\n",
    "\n",
    "La segunda idea clave es que es posible encontrar las matrices de cámara $\\mathsf M_1$ y $\\mathsf M_2$ que han producido una determinada matriz $\\mathsf F$.\n",
    "\n",
    "Veremos cómo hacerlo dentro de un momento pero antes es importante señalar que estas matrices dependen del sistema de referencia del espacio 3D, que para nuestros propósitos ahora es irrelevante. Nos basta con encontrar dos cámaras **compatibles** con la geometría epipolar. Lo importante es la posición relativa entre las cámaras. Por comodidad, una de ellas se pone en el origen de coordenadas. \n",
    "\n",
    "Además, es imposible distinguir un objeto real de una maqueta (perfecta) vista desde más cerca. Partiendo de información puramente geométrica solo se puede conseguir una reconstrucción similar de la escena 3D, no euclídea. No se puede deducir la distancia real entre las cámaras ni el tamaño de los objetos.\n",
    "\n",
    "Pero esa no es la única ambigüedad a la que nos enfrentamos. La imagen $p$ de una escena $P$ tomada con la cámara $\\mathsf M$ es idéntica a la de una transformación proyectiva del espacio $P' = \\mathsf H P$ observada con una cámara $\\mathsf M' =  (\\mathsf M \\mathsf H^{-1})$. Esta transformación $\\mathsf H$ puede incluir deformaciones tremendas, y sin embargo da lugar a la misma matriz fundamental. Por tanto, encontrar cámaras compatibles no es suficiente. Deben ser cámaras que tengan la estructura $\\mathsf K [R \\mid t]$ de una cámara real, compuesta por una matriz de calibración conocida o razonable y una pose (rotación + desplazamiento). Por tanto, si hacemos que el sistema de referencia del mundo coincida con el de la primera cámara:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathsf M_1 &= \\mathsf K_1 [I \\mid 0 ]\\\\\n",
    "\\mathsf M_2 &= \\mathsf K_2 [R \\mid t ]\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "Supongamos inicialmente que hemos obtenido las matrices $\\mathsf K_1$ y $\\mathsf K_2$ en un proceso de calibracion previo. Entonces solo queda por determinar la rotación relativa $R$ y (la dirección de) $t$.\n",
    "\n",
    "Recordemos que la matriz de calibración transforma las coordenadas del plano de imagen, pasando del sistema de referencia \"físico\" a la malla de pixels. La restricción epipolar expresada mediante la matriz fundamental $\\mathsf F$ utiliza coordenadas de pixel $x$, pero puede igualmente expresarse en coordenadas físicas $x_n$ (tambien llamadas normalizadas):\n",
    "\n",
    "$$x = \\mathsf K x_n$$\n",
    "\n",
    "Por tanto la restricción epipolar $ x' \\cdot \\mathsf F x = 0$ es equivalente a $x_n' \\cdot \\mathsf E \\,x_n = 0$, donde la matriz $\\mathsf E$ se llama **matriz esencial** del par estéreo. Está directamente relacionada con $\\mathsf F$:\n",
    "\n",
    "$$ x_n\\cdot \\, \\underbrace{\\mathsf K^\\mathsf T\\, \\mathsf F\\, \\mathsf K}_{\\mathsf E} \\, x_n = 0 $$\n",
    "\n",
    "La matriz esencial tiene una estructura matemática muy simple: $\\mathsf E = t_\\times R $ (donde $t_\\times$ es una matriz que implementa el producto vectorial por $t$), por lo que llegamos al\n",
    "\n",
    "**Resultado 5**:  De la geometría epipolar se pueden extraer las cámaras (precalibradas).\n",
    "\n",
    "\n",
    "### Teorema fundamental de la geometría visual\n",
    "\n",
    "Resumiendo lo anterior:\n",
    "\n",
    "- puntos correspondientes $\\rightarrow$ matriz $\\mathsf F$\n",
    "\n",
    "- matriz $\\mathsf F$ $\\rightarrow$ cámaras\n",
    "\n",
    "- puntos correspondientes $+$ cámaras $\\rightarrow$  puntos 3D\n",
    "\n",
    "En conclusión: **puntos correspondientes  $\\rightarrow$  puntos 3D**.\n",
    "\n",
    "Teniendo en cuenta lo siguiente:\n",
    "\n",
    "- La escala real de la reconstrucción se desconoce (hay que identificar algo de tamaño conocido en la escena).\n",
    "\n",
    "- Necesitamos las matrices de calibración $\\mathsf K$.\n",
    "\n",
    "- Si solo desconocemos el parámetro $f$ hay métodos de **autocalibración** que pueden deducirlo de la matriz $\\mathsf F$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este video musical explica perfectamente este capítulo: [The Fundamental Matrix Song](http://danielwedge.com/fmatrix).\n",
    "\n",
    "Aquí está la explicación de la [geometría epipolar en OpenCV](https://docs.opencv.org/4.7.0/da/de9/tutorial_py_epipolar_geometry.html).\n",
    "\n",
    "(Pendiente: demo interactiva, disponible en docker.)\n",
    "\n",
    "Dentro de un momento veremos un ejemplo numérico de estas ideas con la ayuda de OpenCV. Antes comentaremos brevemente lo que ocurre cuando tenemos más de dos imágenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometría de 3 vistas\n",
    "\n",
    "En la imagen central de la figura siguiente se muestran las rectas epipolares generadas por la proyección $p_1$ en la cámara izquierda, y $p_2$ en la derecha, ambos correspondientes a un mismo punto del espacio. Su intersección nos dice donde debe estar la proyección $p_3$ en la imagen central.\n",
    "\n",
    "<img src=\"../images/demos/transfer.png\" width=\"55%\"/>\n",
    "\n",
    "Esta operación se llama \"epipolar transfer\". Es equivalente a triangular el punto $P$ con las imágenes de ambos lados y después proyectarlo en la imagen central. Hacerlo con epipolar transfer tiene interés porque demuestra que se puede predecir \"directamente\", sin pasar por el espacio 3D, la posición en una tercera imagen de un punto si lo vemos en otras dos imágenes. Esto en la práctica no es muy útil. Sin ir más lejos, el cálculo de la intersección está mál condicionado si las rectas epipolares son casi paraleas. Sin embargo esto indica que existe algún tipo de restricción en la geometría de tres vistas.\n",
    "\n",
    "Efectivamente, esta restricción existe y es mejor que 3 pares de restricciones epipolares. Se llama **restricción trifocal** y la mejor forma de expresarla es con rectas. En la figura siguiente se ven las proyecciones en tres cámaras de una recta en el espacio. Los planos retroproyectados de dos cualquiera de ellas se intersectarán en una recta 3D. La recta proyectada en la tercera imagen queda fijada completamente.\n",
    "\n",
    "<img src=\"../images/demos/trifocal.png\" width=\"55%\"/>\n",
    "\n",
    "La operación lineal que obtiene la línea $l_3$ a partir de $l_1$ y $l_2$ se llama **tensor trifocal** $\\mathcal T$.\n",
    "\n",
    "$$l_3 = \\mathcal T (l_1, l_2)$$\n",
    "\n",
    "(Un tensor es una transformación multilineal, es decir una función de varias variables vectoriales, que se comporta linealmente (como un producto matriz-vector), en cada uno de los argumentos. Se representa mediante un array multidimensional de coeficientes.)\n",
    "\n",
    "El tensor trifocal se puede deducir de un conjunto de tripletas de líneas (o puntos) correspondientes y de él se puede extraer la posición relativa de las cámaras.\n",
    "\n",
    "### Geometría de múltiples vistas\n",
    "\n",
    "Se han estudiado también las configuraciones de 4 y más vistas, hasta llegar al caso general de la reconstrucción 3D a partir de múltiples vistas ([Structure From Motion](https://en.wikipedia.org/wiki/Structure_from_Motion)).\n",
    "\n",
    "<img src=\"../images/demos/sfm.png\" width=\"50%\"/>\n",
    "\n",
    "Es un problema que se considera resuelto tras una época de importantes avances teóricos.\n",
    "\n",
    "El software [VisualSFM](http://ccwu.me/vsfm/) fue un avance muy relevante en su momento.\n",
    "\n",
    "Hay productos comerciales como [Pix4d](https://www.sensefly.com/drones/photogrammetry-software.html) que obtiene modelos 2D y 3D a partir de imágenes de drones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparamos algunas utilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy             as np\n",
    "import cv2               as cv\n",
    "import numpy.linalg      as la\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets          import interactive\n",
    "\n",
    "from umucv.htrans import homog, inhomog, jc, col\n",
    "from umucv.util   import shline\n",
    "\n",
    "def fig(w,h):\n",
    "    plt.figure(figsize=(w,h))\n",
    "\n",
    "def readrgb(file):\n",
    "    return cv.cvtColor( cv.imread('../images/'+file), cv.COLOR_BGR2RGB) \n",
    "\n",
    "def rgb2gray(x):\n",
    "    return cv.cvtColor(x,cv.COLOR_RGB2GRAY)\n",
    "\n",
    "def readgray(file):\n",
    "    return cv.imread('../images/'+file, 0) \n",
    "\n",
    "def imshowg(x):\n",
    "    plt.imshow(x, 'gray')\n",
    "\n",
    "# matriz de calibración sencilla dada la\n",
    "# resolución de la imagen y el fov horizontal en grados\n",
    "def Kfov(sz,hfovd):\n",
    "    hfov = np.radians(hfovd)\n",
    "    f = 1/np.tan(hfov/2)\n",
    "    # print(f)\n",
    "    w,h = sz\n",
    "    w2 = w / 2\n",
    "    h2 = h / 2\n",
    "    return np.array([[f*w2, 0,    w2],\n",
    "                     [0,    f*w2, h2],\n",
    "                     [0,    0,    1 ]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapa de profundidad denso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El caso más simple posible de la visión estéreo ocurre cuando partimos de un **par estéreo rectificado**: los puntos correspondientes están en la misma fila de pixels. (Obtenido por software, o con una cámara estéreo precalibrada)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo típico que aparece en muchos libros\n",
    "x1 = readgray('tsukuba-l.png')\n",
    "x2 = readgray('tsukuba-r.png')\n",
    "\n",
    "fig(12,4)\n",
    "plt.subplot(1,2,1); imshowg(x1)\n",
    "plt.subplot(1,2,2); imshowg(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Superponiendo las dos se aprecia la disparidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshowg(x1//2 + x2//2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos un *stereo matcher* de opencv para determinar la disparidad de cada pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgbm = cv.StereoSGBM_create(0,16,30)\n",
    "disparity = sgbm.compute(x1,x2)\n",
    "imshowg(disparity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si conocemos la separación entre las cámaras podemos convertir estas disparidades en un mapa de profundidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estudio de la geometría epipolar "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un problema más interesante es la reconstrucción 3D cuando desconocemos la posición de las cámaras. En un primer ejercicio vamos a utilizar correspondencias marcadas a mano para verificar los algoritmos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos elegido puntos 3D con una estructura muy regular para comprobar visualmente la corrección de los algoritmos. En la práctica los puntos correspondientes suelen estar dispersos de cualquier manera en el espacio.\n",
    "\n",
    "Cuando la **línea base** (la distancia entre las cámaras) es pequeña se pueden usar detectores de keypoints para encontrar correspondencias automáticamente, como veremos en el ejercicio siguiente. Este experimento es de línea base ancha, donde sería necesario usar \"tracking\" de Lucas-Kanade si quisiera hacerse de forma automática. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = np.array(\n",
    "      [[ 278.,  343.],\n",
    "       [ 335.,  312.],\n",
    "       [ 386.,  279.],\n",
    "       [ 433.,  254.],\n",
    "       [ 270.,  304.],\n",
    "       [ 332.,  272.],\n",
    "       [ 389.,  238.],\n",
    "       [ 434.,  210.],\n",
    "       [ 260.,  253.],\n",
    "       [ 324.,  218.],\n",
    "       [ 389.,  186.],\n",
    "       [ 442.,  160.],\n",
    "       [ 244.,  189.],\n",
    "       [ 317.,  158.],\n",
    "       [ 390.,  125.],\n",
    "       [ 447.,   97.],\n",
    "       [ 204.,  147.],\n",
    "       [ 276.,  114.],\n",
    "       [ 340.,   87.],\n",
    "       [ 395.,   64.],\n",
    "       [ 172.,  107.],\n",
    "       [ 238.,   83.],\n",
    "       [ 302.,   55.],\n",
    "       [ 355.,   39.],\n",
    "       [ 146.,   81.],\n",
    "       [ 210.,   58.],\n",
    "       [ 273.,   37.],\n",
    "       [ 318.,   21.]])\n",
    "\n",
    "v2 = np.array(\n",
    "      [[ 184.,  399.],\n",
    "       [ 226.,  418.],\n",
    "       [ 277.,  434.],\n",
    "       [ 338.,  460.],\n",
    "       [ 171.,  358.],\n",
    "       [ 216.,  374.],\n",
    "       [ 273.,  394.],\n",
    "       [ 331.,  412.],\n",
    "       [ 155.,  307.],\n",
    "       [ 203.,  321.],\n",
    "       [ 262.,  342.],\n",
    "       [ 324.,  358.],\n",
    "       [ 138.,  251.],\n",
    "       [ 191.,  265.],\n",
    "       [ 252.,  284.],\n",
    "       [ 321.,  299.],\n",
    "       [ 186.,  218.],\n",
    "       [ 233.,  227.],\n",
    "       [ 291.,  240.],\n",
    "       [ 357.,  255.],\n",
    "       [ 226.,  188.],\n",
    "       [ 269.,  196.],\n",
    "       [ 327.,  206.],\n",
    "       [ 387.,  220.],\n",
    "       [ 257.,  170.],\n",
    "       [ 299.,  177.],\n",
    "       [ 352.,  185.],\n",
    "       [ 409.,  194.]])\n",
    "\n",
    "rgb1 = readrgb('cube3.png')\n",
    "rgb2 = readrgb('cube4.png')\n",
    "\n",
    "x1 = rgb2gray(rgb1)\n",
    "x2 = rgb2gray(rgb2)\n",
    "\n",
    "fig(12,4)\n",
    "plt.subplot(1,2,1)\n",
    "imshowg(x1); ax = plt.axis()\n",
    "plt.plot(v1[:,0],v1[:,1],'.r'); plt.axis(ax)\n",
    "plt.subplot(1,2,2)\n",
    "imshowg(x2)\n",
    "plt.plot(*v2.T,'.',color='red'); plt.axis(ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando no hay outliers (correspondencias erróneas) la matriz $\\mathsf F$ se puede obtener resolviendo un sencillo sistema de ecuaciones. En la práctica es mejor usar la función  `cv.findFundamentalMat` de OpenCV, que utiliza algoritmos de estimación robustos. La opción `cv.FM_LMEDS` es una alternativa a RANSAC que consigue un resultado parecido en presencia de outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F,_ = cv.findFundamentalMat(v1, v2, cv.FM_LMEDS)\n",
    "F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobemos que la restriccion epipolar $x' \\cdot \\mathsf F x = 0$ se cumple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ x2 @ F @ x1 for x1,x2 in zip(homog(v1),homog(v2)) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido al ruido de medida no son ceros perfectos pero el residuo numérico es razonablemente pequeño."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matriz fundamental nos da las rectas epipolares de cualquier punto (la recta donde debe estar ese punto en la otra imagen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elegimos un punto cualquiera\n",
    "n = 10\n",
    "pt1 = homog(v1[n])\n",
    "pt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculamos su línea epipolar\n",
    "lepi = F @ pt1\n",
    "\n",
    "lepi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig(12,4)\n",
    "plt.subplot(1,2,1)\n",
    "imshowg(x1); ax = plt.axis()\n",
    "plt.plot(*v1[n],'.',color='red'); plt.axis(ax)\n",
    "plt.subplot(1,2,2)\n",
    "imshowg(x2)\n",
    "shline(lepi)\n",
    "plt.axis(ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathsf F^T$ hace el mismo trabajo con la imagen derecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt2 = (160,410,1)  # p.ej., la nariz de Bart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lepi = F.T @ pt2\n",
    "\n",
    "lepi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig(12,4)\n",
    "plt.subplot(1,2,1)\n",
    "imshowg(x1);\n",
    "ax = plt.axis()\n",
    "shline(lepi)\n",
    "plt.axis(ax)\n",
    "plt.subplot(1,2,2)\n",
    "imshowg(x2)\n",
    "plt.plot(pt2[0],pt2[1],'.',color='red'); plt.axis(ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para extraer las cámaras necesitamos la matriz de calibración. (Luego veremos que en ciertos casos se puede *autocalibrar*.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta es una buena aproximación\n",
    "K = Kfov((640,480), 64)\n",
    "K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer paso es formar la matriz Esencial, que \"implementa\" la restricción epipolar con coordenadas normalizadas (en el sistema de referencia físico, no de pixels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = K.T @ F @ K\n",
    "\n",
    "print(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matriz esencial codifica internamente la rotación relativa entre las cámaras y la dirección del desplazamiento entre ellas. Por su estructura matemática debe tener dos [valores singulares](https://en.wikipedia.org/wiki/Singular_value_decomposition) iguales y el tercero nulo.\n",
    "\n",
    "Veamos si nuestra matriz esencial lo cumple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(la.svd(E)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No es una matriz esencial perfecta, pero tiene una calidad muy aceptable.\n",
    "\n",
    "Finalmente, de la matriz Esencial extraemos un par de cámaras consistentes con las imágenes observadas. Pero hay 4 soluciones posibles, de las cuales solo una es físicamente aceptable.\n",
    "\n",
    "La función de OpenCV que implementa este algoritmo es `cv.decomposeEssentialMat`, que devuelve la 4 posibilidades de forma compacta. La forma de desempaquetar las cámaras es la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE1,RE2,t = cv.decomposeEssentialMat(E)\n",
    "\n",
    "# cámara en el origen, es común en las 4 posibilidaes\n",
    "M0 = K @ jc(np.eye(3),col(0,0,0))\n",
    "\n",
    "# 4 posibilidades para la otra cámara\n",
    "# (jc hace una matriz por bloques juntando columnas)\n",
    "M1_1 = K @ jc(RE1,t)\n",
    "M1_2 = K @ jc(RE2,t)\n",
    "M1_3 = K @ jc(RE1,-t)\n",
    "M1_4 = K @ jc(RE2,-t)\n",
    "\n",
    "# Mostramos la primera posibilidad\n",
    "print(M0)\n",
    "\n",
    "print(M1_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probando las 4 posibilidades vemos que en este caso el par M0-M1_4 es el único que triangula todos los puntos delante de las dos cámaras. (Este paso se puede automatizar.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparamos un gráfico 3D\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from umucv.util import cameraOutline\n",
    "\n",
    "def plot3(ax,c,color):\n",
    "    ax.plot(c[:,0],c[:,1],c[:,2],color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de OpenCV que implementa el proceso de triangulación es `cv.triangulatePoints`. Observa que recibe coordenadas cartesianas pero devuelve coordenadas homogéneas (lo que permite recontruir puntos en el infinito). Además los arrays se organizan por columnas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos la primera solución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M1 = M1_1\n",
    "\n",
    "p3d = cv.triangulatePoints(M0, M1, v1.T, v2.T)\n",
    "p3d = inhomog(p3d.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(a=90):\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    plot3(ax,cameraOutline(M0),'blue');\n",
    "    plot3(ax,cameraOutline(M1),'red');\n",
    "    plot3(ax,p3d,'.g')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlabel('X'); ax.set_ylabel('Y'); ax.set_zlabel('Z')\n",
    "    ax.view_init(elev=30., azim=a)\n",
    "\n",
    "interactive(fun, a = (-180,180,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta solución no es válida porque algunos puntos quedan por detrás las cámaras. Probando las demás vemos que la correcta en este caso el la tercera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M1 = M1_3\n",
    "\n",
    "p3d = cv.triangulatePoints(M0, M1, v1.T, v2.T)\n",
    "p3d = inhomog(p3d.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(a=90):\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    plot3(ax,cameraOutline(M0),'blue');\n",
    "    plot3(ax,cameraOutline(M1),'red');\n",
    "    plot3(ax,p3d,'.g')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlabel('X'); ax.set_ylabel('Y'); ax.set_zlabel('Z')\n",
    "    ax.view_init(elev=30., azim=a)\n",
    "\n",
    "interactive(fun, a = (-180,180,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambiando del punto de vista se observa que los puntos 3D reconstruidos están claramente en dos planos perpendiculares de acuerdo con la estructura del cubo de Rubik."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En una aplicación real el paso siguiente es una **rectificación estéreo**. Se aplica una homografía a las imágenes para que queden igual que si se hubieran tomado con un par estéreo alineado, lo que permite obtener un mapa de profundidad con el stereo matcher utilizado en el experimento inicial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocalibración\n",
    "\n",
    "Si desconocemos la matriz de calibración $\\mathsf K$ podemos buscar una aproximación que dé lugar a una matriz esencial $\\mathsf E$ geométricamente válida. (Suponemos la misma $\\mathsf K$ en las dos vistas.)\n",
    "\n",
    "Si como es usual la cámara tiene pixel cuadrado, solo merece la pena explorar el parámetro $f$ de la cámara (a través del FOV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# si el fov es correcto los dos valores singulares de E = KT F K serán iguales\n",
    "def quality(F, fov):\n",
    "    K = Kfov((640,480),fov)\n",
    "    E = K.T @ F @ K\n",
    "    s1,s2,_ = la.svd(E)[1]\n",
    "    return (s1-s2)/s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality(F, 80), quality(F, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos todos los valores del campo visual en un rango amplio (p. ej. entre 20 y 130 grados) para ver cuál recupera mejor la matriz esencial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fovs = np.arange(20,130,0.5)\n",
    "Qs = [quality(F, f) for f in fovs]\n",
    "plt.plot(fovs,Qs);\n",
    "plt.xlabel('FOV (grados)');\n",
    "plt.ylabel('Error de E');\n",
    "plt.ylim(0,0.5);\n",
    "\n",
    "print(fovs[np.argmin(Qs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si la rotación relativa entre las imágenes es pequeña este método será muy impreciso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La posibilidad de autocalibración es un resultado teórico muy importante. En principio se puede resolver el ejercicio FOV simplemente moviendo la cámara, sin necesidad de insertar en la escena ningún objeto de referencia. Pero en la práctica la autocalibración es muy sensible al ruido de medida. Es recomendable trabajar con cámaras precalibradas siempre que sea posible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segundo ejemplo\n",
    "\n",
    "Con \"línea base\" pequeña podemos obtener F automáticamente con puntos de interés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb1 = readrgb('left.png')\n",
    "rgb2 = readrgb('right.png')\n",
    "\n",
    "x1 = rgb2gray(rgb1)\n",
    "x2 = rgb2gray(rgb2)\n",
    "\n",
    "fig(12,4)\n",
    "imshowg(np.hstack([rgb1,rgb2]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv.SIFT_create()\n",
    "(k1, d1) = sift.detectAndCompute(x1, None)\n",
    "(k2, d2) = sift.detectAndCompute(x2, None)\n",
    "print(len(k1),len(k2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig(12,4)\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow( cv.drawKeypoints(image=x1,\n",
    "                             outImage=None,\n",
    "                             keypoints=k1,\n",
    "                             flags=4, color = (128,0,0)) );\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow( cv.drawKeypoints(image=x2,\n",
    "                             outImage=None,\n",
    "                             keypoints=k2,\n",
    "                             flags=4, color = (128,0,0)) );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf = cv.BFMatcher()\n",
    "\n",
    "matches = bf.knnMatch(d2,d1,k=2)\n",
    "\n",
    "len(matches)\n",
    "\n",
    "# Apply ratio test\n",
    "good = []\n",
    "for mt in matches:\n",
    "    if len(mt) == 2:\n",
    "        best, second = mt\n",
    "        if best.distance < 0.75*second.distance:\n",
    "            good.append(best) \n",
    "\n",
    "print(len(good))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img3 = cv.drawMatches(x2,k2,\n",
    "                      x1,k1,\n",
    "                      good,\n",
    "                      flags=2,outImg=None,\n",
    "                      matchColor=(128,0,0))\n",
    "fig(12,4)\n",
    "plt.imshow(img3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como siempre ocurre en estos casos algunas correspondencias son erróneas pero se eliminarán al no cumplir la restricción epipolar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ponemos los puntos en el formato admitido por cv.findFundamentalMat\n",
    "p1r = np.array([ k1[m.trainIdx].pt for m in good ]).astype(np.float32).reshape(-1,2)\n",
    "p2r = np.array([ k2[m.queryIdx].pt for m in good ]).astype(np.float32).reshape(-1,2)\n",
    "\n",
    "# usamos método robusto frente a correspondencias erróneas (\"outliers\")\n",
    "F, mask = cv.findFundamentalMat(p1r,p2r,cv.FM_LMEDS)\n",
    "\n",
    "F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionamos las correspondencias \"buenas\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = mask.ravel() > 0  # viene en formato raro, lo pasamos a array de bool\n",
    "\n",
    "# inliers\n",
    "p1 = p1r[mask]\n",
    "p2 = p2r[mask]\n",
    "\n",
    "ok = [ x for x,ok in zip(good,mask) if ok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig(12,4)\n",
    "img4 = cv.drawMatches(x2,k2,x1,k1,ok,flags=2,outImg=None,matchColor=(0,255,0))\n",
    "plt.imshow(img4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podríamos continuar como en el ejercicio anterior: obtener matriz esencial, cámaras candidatas y triangulación.\n",
    "\n",
    "Pero en este caso vamos a intentar obtener un mapa de disparidad denso, rectificando antes las imágenes para que los puntos correspondientes estén en la misma fila de pixels y cercanos entre sí. Hay un método calibrado y otro no calibrado. Probaremos este último."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,H1,H2 = cv.stereoRectifyUncalibrated(p1,p2,F,(640,480))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1r = cv.warpPerspective(rgb1,H1,(640,480))\n",
    "x2r = cv.warpPerspective(rgb2,H2,(640,480))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig(12,4)\n",
    "imshowg(np.hstack([x1r,x2r]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las superponemos para comprobar visualmente que los puntos correspondientes están a la misma altura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig(8,8)\n",
    "xm=(x1r//2+x2r//2)\n",
    "imshowg(xm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado parece correcto: en todos los casos los puntos correspondientes están en la misma fila y relativamente cerca, lo que facilita el cálculo de disparidad densa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuará..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen de la teoría"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deseamos obtener la estructura 3D de una escena a partir de dos imágenes.\n",
    "\n",
    "Es necesario que los centros de proyección estén separados en el espacio. Si no es así las imágenes se relacionan por una homografía (son esencialmente equivalentes), no hay disparidad.\n",
    "\n",
    "Si conocemos las matrices de cámara podemos hacer triangulación de puntos correspondientes para deducir su posición en el espacio.\n",
    "\n",
    "La triangulación es muy simple si las cámaras están alineadas. En este caso se puede intentar obtener un mapa de profundidad denso.\n",
    "\n",
    "Si la escena contiene un objeto de referencia podemos estimar fácilmente las matrices de cámara.\n",
    "\n",
    "Pero en muchos casos desconocemos todo: las matrices de cámara y la escena. (Cuando sea posible es muy conveniente tener cámaras precalibradas.)\n",
    "\n",
    "El resultado fundamental de la geometría de dos vistas es que los puntos correspondientes tienen que estar en las líneas epipolares.\n",
    "\n",
    "Esta \"restricción epipolar\" es la consecuencia de que los rayos ópticos de puntos correspondientes y la línea base forman un triángulo en el espacio.\n",
    "\n",
    "Esa condición nos permite descartar correspondencias erróneas.\n",
    "\n",
    "Gracias a que trabajamos con coordenadas homogéneas, las líneas epipolares se consiguen mediante una transformación lineal cuya matriz de coeficientes es la **matriz fundamental** $\\mathsf F$ del par estéreo. $\\mathsf F$ se puede construir fácilmente a partir de las cámaras.\n",
    "\n",
    "Los puntos correspondientes cumplen $x' \\cdot \\mathsf F x = 0$.\n",
    "\n",
    "A partir de puntos correspondientes podemos encontrar $\\mathsf F$.\n",
    "\n",
    "A partir de $\\mathsf F$ podemos encontrar cámaras compatibles, que nos permiten triangular los puntos correspondientes.\n",
    "\n",
    "Si desconocemos las matrices de calibración $\\mathsf K$ la reconstrucción tiene ambigüedad proyectiva. Existen deformaciones del espacio que, con cámaras deformadas \"inversamente\", producen la misma geometría epipolar $\\mathsf F$.\n",
    "\n",
    "Esta solución tiene en general poca utilidad. Normalmente nos interesa una solución que solo tenga ambiguedad de escala.\n",
    "\n",
    "Si conocemos las $\\mathsf K$ la restricción epipolar sobre pixels crudos puede expresarse sobre coordenadas normalizadas (imagen óptica ideal) por medio de la **matriz esencial** $\\mathsf E$ del par estéreo. Los puntos normalizados cumplen $x_n' \\cdot \\mathsf E x_n = 0$.\n",
    "\n",
    "La matriz esencial depende solo de la dirección del desplazamiento $t$ y de la orientación relativa $R$ entre las dos cámaras.\n",
    "\n",
    "$R$ y $t$ se pueden extraer fácilmente de la matriz esencial estimada, lo que permite encontrar \"cámaras reales\", con la única ambigüedad de la longitud de la línea base. Por tanto la reconstrucción solo tendrá ambigüedad de escala.\n",
    "\n",
    "Hay 4 soluciones posibles, de las cuales solo una tiene sentido físico (la que reconstruye (triangula) los puntos por delante).\n",
    "\n",
    "En conclusión, $\\mathsf K$ + puntos correspondientes $\\rightarrow$ posición de las cámaras  + estructura 3D de la escena. Es lo que se conoce como el problema *SfM* (structure from motion) o *SLAM* simultaneous localization and mapping.\n",
    "\n",
    "Si desconocemos $\\mathsf K$ no todo está perdido: existen métodos de autocalibración."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "state": {
    "ee1821a61508409a91e735f1da5dad83": {
     "views": [
      {
       "cell_index": 37
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
